{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager\n",
    "from collections.abc import AsyncIterator\n",
    "\n",
    "from fake_database import Database  # Replace with your actual DB type\n",
    "\n",
    "from mcp.server import Server\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def server_lifespan(server: Server) -> AsyncIterator[dict]:\n",
    "    \"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n",
    "    # Initialize resources on startup\n",
    "    db = await Database.connect()\n",
    "    try:\n",
    "        yield {\"db\": db}\n",
    "    finally:\n",
    "        # Clean up on shutdown\n",
    "        await db.disconnect()\n",
    "\n",
    "\n",
    "# Pass lifespan to server\n",
    "server = Server(\"example-server\", lifespan=server_lifespan)\n",
    "\n",
    "\n",
    "# Access lifespan context in handlers\n",
    "@server.call_tool()\n",
    "async def query_db(name: str, arguments: dict) -> list:\n",
    "    ctx = server.request_context\n",
    "    db = ctx.lifespan_context[\"db\"]\n",
    "    return await db.query(arguments[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_onenote_content(self):\n",
    "        \"\"\"Load content from OneNote notebooks.\"\"\"\n",
    "        if not self.onenote_token:\n",
    "            print(\"[DEBUG] No existing OneNote token found. Acquiring new token...\")\n",
    "            self.get_onenote_token_interactive()\n",
    "\n",
    "        # Initialize database\n",
    "        init_db()\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.onenote_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        graph_api = \"https://graph.microsoft.com/v1.0\"\n",
    "\n",
    "        # First, get user info\n",
    "        user_response = self.make_request_with_retry(f\"{graph_api}/me\", headers)\n",
    "        if not user_response:\n",
    "            raise Exception(\"Failed to get user info after retries\")\n",
    "        \n",
    "        user_info = user_response.json()\n",
    "        user_display_name = user_info.get('displayName', 'Unknown User')\n",
    "\n",
    "        onenote_docs = []\n",
    "        notebooks_response = self.make_request_with_retry(f\"{graph_api}/me/onenote/notebooks\", headers)\n",
    "        if not notebooks_response:\n",
    "            print(\"[DEBUG] Failed to retrieve notebooks after retries\")\n",
    "            return []\n",
    "\n",
    "        notebooks = notebooks_response.json().get(\"value\", [])\n",
    "        print(f\"[DEBUG] Found {len(notebooks)} notebooks.\")\n",
    "\n",
    "        for notebook in notebooks:\n",
    "            notebook_id = notebook[\"id\"]\n",
    "            notebook_name = notebook[\"displayName\"]\n",
    "            print(f\"[DEBUG] Processing notebook: {notebook_name} (ID: {notebook_id})\")\n",
    "\n",
    "            sections_response = self.make_request_with_retry(f\"{graph_api}/me/onenote/notebooks/{notebook_id}/sections\", headers)\n",
    "            if not sections_response:\n",
    "                print(f\"[DEBUG] Failed to retrieve sections for notebook {notebook_name} after retries\")\n",
    "                continue\n",
    "\n",
    "            sections = sections_response.json().get(\"value\", [])\n",
    "            print(f\"[DEBUG] Found {len(sections)} sections in notebook {notebook_name}.\")\n",
    "\n",
    "            for section in sections:\n",
    "                section_id = section[\"id\"]\n",
    "                section_name = section[\"displayName\"]\n",
    "                print(f\"[DEBUG] Processing section: {section_name} (ID: {section_id})\")\n",
    "\n",
    "                pages_response = self.make_request_with_retry(f\"{graph_api}/me/onenote/sections/{section_id}/pages\", headers)\n",
    "                if not pages_response:\n",
    "                    print(f\"[DEBUG] Failed to retrieve pages for section {section_name} after retries\")\n",
    "                    continue\n",
    "\n",
    "                pages = pages_response.json().get(\"value\", [])\n",
    "                print(f\"[DEBUG] Found {len(pages)} pages in section {section_name}.\")\n",
    "\n",
    "                for page in pages:\n",
    "                    page_id = page[\"id\"]\n",
    "                    page_title = page[\"title\"]\n",
    "                    print(f\"[DEBUG] Processing page: {page_title} (ID: {page_id})\")\n",
    "\n",
    "                    # Get page metadata to check last modified time\n",
    "                    page_metadata_response = self.make_request_with_retry(f\"{graph_api}/me/onenote/pages/{page_id}\", headers)\n",
    "                    if not page_metadata_response:\n",
    "                        print(f\"[DEBUG] Failed to get page metadata for {page_title} after retries\")\n",
    "                        continue\n",
    "\n",
    "                    page_metadata = page_metadata_response.json()\n",
    "                    last_modified = page_metadata.get('lastModifiedDateTime')\n",
    "                    \n",
    "                    # Check if page exists and has the same last_modified time\n",
    "                    db_last_modified = get_page_last_modified(page_id)\n",
    "                    if db_last_modified and db_last_modified == last_modified:\n",
    "                        print(f\"[DEBUG] Page {page_title} exists and hasn't been modified, skipping...\")\n",
    "                        # Get content from database\n",
    "                        content = get_page_content(page_id)\n",
    "                        if content:\n",
    "                            onenote_docs.append(Document(\n",
    "                                page_content=content,\n",
    "                                metadata={\n",
    "                                    \"source\": f\"onenote://{user_display_name}/{notebook_name}/{section_name}/{page_title}\",\n",
    "                                    \"type\": \"onenote\",\n",
    "                                    \"user\": user_display_name,\n",
    "                                    \"notebook\": notebook_name,\n",
    "                                    \"section\": section_name,\n",
    "                                    \"page\": page_title\n",
    "                                }\n",
    "                            ))\n",
    "                        continue\n",
    "                    else:\n",
    "                        if db_last_modified:\n",
    "                            print(f\"[DEBUG] Page {page_title} has been modified, reprocessing...\")\n",
    "                        else:\n",
    "                            print(f\"[DEBUG] New page {page_title}, processing...\")\n",
    "\n",
    "                    content_response = self.make_request_with_retry(f\"{graph_api}/me/onenote/pages/{page_id}/content\", headers)\n",
    "                    if content_response and content_response.status_code == 200:\n",
    "                        html_content = content_response.text\n",
    "                        import re\n",
    "                        text_content = re.sub('<[^<]+?>', ' ', html_content).strip()\n",
    "\n",
    "                        # Save page to database with new last_modified time\n",
    "                        page_data = {\n",
    "                            'page_id': page_id,\n",
    "                            'notebook_id': notebook_id,\n",
    "                            'notebook_name': notebook_name,\n",
    "                            'section_id': section_id,\n",
    "                            'section_name': section_name,\n",
    "                            'page_title': page_title,\n",
    "                            'content': text_content,\n",
    "                            'last_modified': last_modified\n",
    "                        }\n",
    "                        save_page(page_data)\n",
    "\n",
    "                        onenote_docs.append(Document(\n",
    "                            page_content=text_content,\n",
    "                            metadata={\n",
    "                                \"source\": f\"onenote://{user_display_name}/{notebook_name}/{section_name}/{page_title}\",\n",
    "                                \"type\": \"onenote\",\n",
    "                                \"user\": user_display_name,\n",
    "                                \"notebook\": notebook_name,\n",
    "                                \"section\": section_name,\n",
    "                                \"page\": page_title\n",
    "                            }\n",
    "                        ))\n",
    "                        print(f\"[DEBUG] Successfully processed and added page: {page_title}\")\n",
    "                    else:\n",
    "                        print(f\"[DEBUG] Failed to retrieve content for page {page_title} after retries\")\n",
    "\n",
    "            # Add a small delay between notebooks to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(f\"[DEBUG] Total OneNote documents loaded: {len(onenote_docs)}\")\n",
    "        self.documents.extend(onenote_docs)\n",
    "        return onenote_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def get_onenote_token_interactive(self):\n",
    "        \"\"\"\n",
    "        Get OneNote token using interactive authentication.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            app = msal.PublicClientApplication(\n",
    "                self.microsoft_client_id,\n",
    "                authority=\"https://login.microsoftonline.com/consumers\"\n",
    "            )\n",
    "\n",
    "            # Define the scopes - use the correct format for Microsoft Graph API\n",
    "            scopes = [\"https://graph.microsoft.com/Notes.Read.All\",\n",
    "                     \"https://graph.microsoft.com/Notes.Read\",\n",
    "                     \"https://graph.microsoft.com/User.Read\"]\n",
    "\n",
    "            print(\"[DEBUG] No cached token found, initiating interactive authentication...\")\n",
    "            result = app.acquire_token_interactive(scopes=scopes)\n",
    "\n",
    "            if \"access_token\" in result:\n",
    "                self.onenote_token = result[\"access_token\"]\n",
    "                print(\"[DEBUG] Successfully acquired access token\")\n",
    "                return self.onenote_token\n",
    "            else:\n",
    "                print(\"[ERROR] Interactive authentication failed:\", result.get(\"error_description\", \"Unknown error\"))\n",
    "                print(\"\\nPlease verify in Azure Portal:\")\n",
    "                print(\"1. Go to Azure Portal > App Registrations > Your App\")\n",
    "                print(\"2. Click on 'Authentication' in the left menu\")\n",
    "                print(\"3. Under 'Platform configurations', ensure 'Mobile and desktop applications' is added\")\n",
    "                print(\"4. Check the box for 'https://login.microsoftonline.com/common/oauth2/nativeclient'\")\n",
    "                print(\"5. Under 'Default client type', ensure 'Yes' is selected for 'Treat application as a public client'\")\n",
    "                print(\"6. Under 'API permissions', ensure you have:\")\n",
    "                print(\"   - User.Read\")\n",
    "                print(\"   - Notes.Read.All\")\n",
    "                print(\"   - Notes.Read\")\n",
    "                print(\"\\nDebug information:\")\n",
    "                print(f\"Client ID: {self.microsoft_client_id[:10]}... (truncated)\")\n",
    "                print(f\"Authority: https://login.microsoftonline.com/consumers\")\n",
    "                raise ConfigurationError(str(result.get(\"error_description\", \"Unknown error\")))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nPlease verify your Azure AD app configuration and try again.\")\n",
    "            raise ConfigurationError(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "data_ingestion.py - Data ingestion pipeline for personal RAG\n",
    "\"\"\"\n",
    "import os\n",
    "# Set the environment variable before importing any HuggingFace libraries\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from typing import List, Dict, Any, Tuple, Optional, Set, Literal\n",
    "import json\n",
    "import cv2\n",
    "import whisper\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, Wav2Vec2Processor, Wav2Vec2Model\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from github import Github\n",
    "import msal\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import subprocess\n",
    "import dateutil.parser\n",
    "import re\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "import signal\n",
    "import uuid\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import librosa\n",
    "\n",
    "# LangChain document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader, \n",
    "    Docx2txtLoader, \n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    CSVLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup signal handler for timeouts\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Operation timed out\")\n",
    "    \n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('data_ingestion.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ConfigurationError(Exception):\n",
    "    \"\"\"Exception raised for configuration-related errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class EmbeddingType(str, Enum):\n",
    "    \"\"\"Types of embeddings that can be generated.\"\"\"\n",
    "    CLIP_TEXT = \"clip_text\"\n",
    "    CLIP_IMAGE = \"clip_image\"\n",
    "    WAV2VEC = \"wav2vec\"\n",
    "\n",
    "@dataclass\n",
    "class DocumentTypeConfig:\n",
    "    \"\"\"Configuration for a specific document type.\"\"\"\n",
    "    extensions: Set[str]\n",
    "    embedding_type: EmbeddingType\n",
    "    description: str\n",
    "    enabled: bool = True\n",
    "\n",
    "@dataclass\n",
    "class DataSourceConfig:\n",
    "    \"\"\"Configuration for a data source.\"\"\"\n",
    "    name: str\n",
    "    enabled: bool\n",
    "    document_types: Dict[str, DocumentTypeConfig]\n",
    "\n",
    "    @classmethod\n",
    "    def default_local_config(cls) -> 'DataSourceConfig':\n",
    "        \"\"\"Create default configuration for local files.\"\"\"\n",
    "        return cls(\n",
    "            name=\"local\",\n",
    "            enabled=True,\n",
    "            document_types={\n",
    "                \"document\": DocumentTypeConfig(\n",
    "                    extensions={'.pdf', '.doc', '.docx', '.txt', '.rtf'},\n",
    "                    embedding_type=EmbeddingType.CLIP_TEXT,\n",
    "                    description=\"Text documents\"\n",
    "                ),\n",
    "                \"video_frame\": DocumentTypeConfig(\n",
    "                    extensions={'.mp4', '.avi', '.mov', '.mkv'},\n",
    "                    embedding_type=EmbeddingType.CLIP_IMAGE,\n",
    "                    description=\"Video files processed as frames\"\n",
    "                ),\n",
    "                \"audio\": DocumentTypeConfig(\n",
    "                    extensions={'.mp3', '.wav', '.m4a', '.flac'},\n",
    "                    embedding_type=EmbeddingType.WAV2VEC,\n",
    "                    description=\"Audio files\"\n",
    "                ),\n",
    "                \"image\": DocumentTypeConfig(\n",
    "                    extensions={'.jpg', '.jpeg', '.png', '.gif', '.bmp'},\n",
    "                    embedding_type=EmbeddingType.CLIP_IMAGE,\n",
    "                    description=\"Image files\"\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def default_github_config(cls) -> 'DataSourceConfig':\n",
    "        \"\"\"Create default configuration for GitHub files.\"\"\"\n",
    "        return cls(\n",
    "            name=\"github\",\n",
    "            enabled=True,\n",
    "            document_types={\n",
    "                \"code\": DocumentTypeConfig(\n",
    "                    extensions={'.py', '.js', '.java', '.cpp', '.go', '.rs'},\n",
    "                    embedding_type=EmbeddingType.CLIP_TEXT,\n",
    "                    description=\"Programming language files\"\n",
    "                ),\n",
    "                \"document\": DocumentTypeConfig(\n",
    "                    extensions={'.md', '.txt', '.rst', '.adoc'},\n",
    "                    embedding_type=EmbeddingType.CLIP_TEXT,\n",
    "                    description=\"Documentation files\"\n",
    "                ),\n",
    "                \"config\": DocumentTypeConfig(\n",
    "                    extensions={'.yml', '.yaml', '.json', '.toml'},\n",
    "                    embedding_type=EmbeddingType.CLIP_TEXT,\n",
    "                    description=\"Configuration files\"\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def default_onenote_config(cls) -> 'DataSourceConfig':\n",
    "        \"\"\"Create default configuration for OneNote.\"\"\"\n",
    "        return cls(\n",
    "            name=\"onenote\",\n",
    "            enabled=True,\n",
    "            document_types={\n",
    "                \"note\": DocumentTypeConfig(\n",
    "                    extensions={'.one'},  # Virtual extension for OneNote pages\n",
    "                    embedding_type=EmbeddingType.CLIP_TEXT,\n",
    "                    description=\"OneNote pages and notebooks\"\n",
    "                ),\n",
    "                \"image_attachment\": DocumentTypeConfig(\n",
    "                    extensions={'.jpg', '.jpeg', '.png', '.gif', '.bmp'},\n",
    "                    embedding_type=EmbeddingType.CLIP_IMAGE,\n",
    "                    description=\"OneNote image attachments\"\n",
    "                ),\n",
    "                \"video_attachment\": DocumentTypeConfig(\n",
    "                    extensions={'.mp4', '.avi', '.mov', '.mkv'},\n",
    "                    embedding_type=EmbeddingType.CLIP_IMAGE,\n",
    "                    description=\"OneNote video attachments\"\n",
    "                ),\n",
    "                \"audio_attachment\": DocumentTypeConfig(\n",
    "                    extensions={'.mp3', '.wav', '.m4a', '.flac'},\n",
    "                    embedding_type=EmbeddingType.WAV2VEC,\n",
    "                    description=\"OneNote audio attachments\"\n",
    "                ),\n",
    "                \"document_attachment\": DocumentTypeConfig(\n",
    "                    extensions={'.pdf', '.doc', '.docx', '.txt', '.rtf'},\n",
    "                    embedding_type=EmbeddingType.CLIP_TEXT,\n",
    "                    description=\"OneNote document attachments\"\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class SourcesConfig:\n",
    "    \"\"\"Configuration for all data sources.\"\"\"\n",
    "    sources: Dict[str, DataSourceConfig] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_file: str) -> 'SourcesConfig':\n",
    "        \"\"\"Create configuration from a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            sources = {}\n",
    "            for source_name, source_data in data.items():\n",
    "                document_types = {}\n",
    "                for doc_type, doc_config in source_data.get('document_types', {}).items():\n",
    "                    document_types[doc_type] = DocumentTypeConfig(\n",
    "                        extensions=set(doc_config.get('extensions', [])),\n",
    "                        embedding_type=EmbeddingType(doc_config.get('embedding_type')),\n",
    "                        description=doc_config.get('description', ''),\n",
    "                        enabled=doc_config.get('enabled', True)\n",
    "                    )\n",
    "                \n",
    "                sources[source_name] = DataSourceConfig(\n",
    "                    name=source_name,\n",
    "                    enabled=source_data.get('enabled', True),\n",
    "                    document_types=document_types\n",
    "                )\n",
    "            \n",
    "            return cls(sources=sources)\n",
    "            \n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            print(f\"Error loading config file {json_file}: {str(e)}\")\n",
    "            print(\"Using default configuration\")\n",
    "            return cls.get_default_config()\n",
    "\n",
    "    @classmethod\n",
    "    def get_default_config(cls) -> 'SourcesConfig':\n",
    "        \"\"\"Get default configuration for all sources.\"\"\"\n",
    "        return cls(sources={\n",
    "            \"local\": DataSourceConfig.default_local_config(),\n",
    "            \"github\": DataSourceConfig.default_github_config(),\n",
    "            \"onenote\": DataSourceConfig.default_onenote_config()\n",
    "        })\n",
    "\n",
    "    def to_json(self, json_file: str):\n",
    "        \"\"\"Save configuration to a JSON file.\"\"\"\n",
    "        data = {}\n",
    "        for source_name, source_config in self.sources.items():\n",
    "            document_types = {}\n",
    "            for doc_type, doc_config in source_config.document_types.items():\n",
    "                document_types[doc_type] = {\n",
    "                    'extensions': list(doc_config.extensions),\n",
    "                    'embedding_type': doc_config.embedding_type.value,\n",
    "                    'description': doc_config.description,\n",
    "                    'enabled': doc_config.enabled\n",
    "                }\n",
    "            \n",
    "            data[source_name] = {\n",
    "                'name': source_config.name,\n",
    "                'enabled': source_config.enabled,\n",
    "                'document_types': document_types\n",
    "            }\n",
    "        \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def is_supported(self, file_extension: str, source: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if a file extension is supported by a source.\"\"\"\n",
    "        if source not in self.sources or not self.sources[source].enabled:\n",
    "            return False, None\n",
    "\n",
    "        file_extension = file_extension.lower()\n",
    "        for doc_type, config in self.sources[source].document_types.items():\n",
    "            if config.enabled and file_extension in config.extensions:\n",
    "                return True, doc_type\n",
    "        return False, None\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, openai_api_key: str, github_token: str = None, \n",
    "                 microsoft_client_id: str = None, microsoft_client_secret: str = None,\n",
    "                 microsoft_tenant_id: str = None, config_file: str = None):\n",
    "        \"\"\"Initialize the DataIngestion pipeline.\n",
    "        \n",
    "        This class handles:\n",
    "        1. Data Ingestion: Parse documents (LangChain loaders), extract frames (OpenCV), transcribe audio (Whisper)\n",
    "        2. Embedding Generation: CLIP for text/images/videos, Wav2Vec for audio\n",
    "        3. Vector Database: Milvus with HNSW indexing\n",
    "        \"\"\"\n",
    "        # Initialize credentials\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.github_token = github_token\n",
    "        self.microsoft_client_id = microsoft_client_id\n",
    "        self.microsoft_client_secret = microsoft_client_secret\n",
    "        self.microsoft_tenant_id = microsoft_tenant_id\n",
    "        self.onenote_token = None\n",
    "\n",
    "        # Load data source configuration\n",
    "        self.config = SourcesConfig.from_json(config_file) if config_file else SourcesConfig.get_default_config()\n",
    "\n",
    "        # Initialize models\n",
    "        self._init_embedding_models()\n",
    "        \n",
    "        # Setup vector database\n",
    "        self.setup_milvus()\n",
    "        \n",
    "    def _init_embedding_models(self):\n",
    "        \"\"\"Initialize embedding models for different content types.\"\"\"\n",
    "        logger.info(\"Loading embedding models...\")\n",
    "        # CLIP for text, images, and videos\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Wav2Vec for audio\n",
    "        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        \n",
    "        # Whisper for audio transcription\n",
    "        self.whisper_model = whisper.load_model(\"base\")\n",
    "        logger.info(\"All embedding models loaded successfully\")\n",
    "        \n",
    "    # ----- DATA EXTRACTION METHODS -----\n",
    "    \n",
    "    def process_document(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process document using LangChain's document loaders.\n",
    "        \n",
    "        Uses appropriate LangChain loaders based on file extension,\n",
    "        with fallbacks for unsupported formats.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing extracted content and metadata\n",
    "        \"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Check if file type is supported in configuration\n",
    "        is_supported, doc_type = self.config.is_supported(file_extension, \"local\")\n",
    "        if not is_supported:\n",
    "            logger.warning(f\"Unsupported file type: {file_extension}\")\n",
    "            return {'content': f\"Unsupported file type: {file_extension}\", \n",
    "                    'metadata': {'error': 'unsupported_file_type'}}\n",
    "        \n",
    "        # Get the document type configuration\n",
    "        doc_config = self.config.sources[\"local\"].document_types[doc_type]\n",
    "        logger.info(f\"Processing {file_name} as {doc_type} with embedding type {doc_config.embedding_type}\")\n",
    "        \n",
    "        # For non-document types that document loaders can't process well, use specialized methods\n",
    "        if doc_type == \"audio\":\n",
    "            return self._process_audio(file_path)\n",
    "        elif doc_type == \"image\":\n",
    "            return self._process_image(file_path)\n",
    "        elif doc_type == \"video_frame\":\n",
    "            return self._process_video_metadata(file_path)\n",
    "        \n",
    "        # Use LangChain document loaders based on file extension\n",
    "        content = None\n",
    "        metadata = {'file_path': file_path}\n",
    "        extraction_method = 'failed'\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Processing document with LangChain: {file_name}\")\n",
    "            \n",
    "            if file_extension == '.pdf':\n",
    "                # Process PDF files with PyPDFLoader\n",
    "                logger.info(f\"Using PyPDFLoader for: {file_name}\")\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Combine all pages into one document\n",
    "                content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                # Add metadata from first page\n",
    "                if documents and hasattr(documents[0], 'metadata'):\n",
    "                    metadata.update(documents[0].metadata)\n",
    "                    metadata['page_count'] = len(documents)\n",
    "                \n",
    "                if content and content.strip():\n",
    "                    logger.info(f\"Successfully extracted content with PyPDFLoader: {len(content)} characters\")\n",
    "                    extraction_method = 'pypdf_langchain'\n",
    "                \n",
    "            elif file_extension in ['.docx', '.doc']:\n",
    "                # Process Word documents\n",
    "                logger.info(f\"Using Docx2txtLoader for: {file_name}\")\n",
    "                try:\n",
    "                    # Try Docx2txtLoader first (for .docx)\n",
    "                    loader = Docx2txtLoader(file_path)\n",
    "                    documents = loader.load()\n",
    "                    extraction_method = 'docx2txt'\n",
    "                except Exception as e:\n",
    "                    # Fall back to UnstructuredWordDocumentLoader for older .doc files\n",
    "                    logger.warning(f\"Docx2txtLoader failed, trying UnstructuredWordDocumentLoader: {str(e)}\")\n",
    "                    loader = UnstructuredWordDocumentLoader(file_path)\n",
    "                    documents = loader.load()\n",
    "                    extraction_method = 'unstructured_word'\n",
    "                \n",
    "                # Combine all parts into one document\n",
    "                content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                # Add metadata from first document\n",
    "                if documents and hasattr(documents[0], 'metadata'):\n",
    "                    metadata.update(documents[0].metadata)\n",
    "                \n",
    "            elif file_extension in ['.html', '.htm']:\n",
    "                # Process HTML files\n",
    "                logger.info(f\"Using UnstructuredHTMLLoader for: {file_name}\")\n",
    "                loader = UnstructuredHTMLLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                if documents and hasattr(documents[0], 'metadata'):\n",
    "                    metadata.update(documents[0].metadata)\n",
    "                extraction_method = 'unstructured_html'\n",
    "                \n",
    "            elif file_extension in ['.md', '.markdown']:\n",
    "                # Process Markdown files\n",
    "                logger.info(f\"Using UnstructuredMarkdownLoader for: {file_name}\")\n",
    "                loader = UnstructuredMarkdownLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                if documents and hasattr(documents[0], 'metadata'):\n",
    "                    metadata.update(documents[0].metadata)\n",
    "                extraction_method = 'unstructured_markdown'\n",
    "                \n",
    "            elif file_extension == '.txt':\n",
    "                # Process plain text files with encoding detection\n",
    "                logger.info(f\"Using TextLoader for: {file_name}\")\n",
    "                # Try multiple encodings\n",
    "                encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'ascii']\n",
    "                for encoding in encodings_to_try:\n",
    "                    try:\n",
    "                        loader = TextLoader(file_path, encoding=encoding)\n",
    "                        documents = loader.load()\n",
    "                        content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                        if documents and hasattr(documents[0], 'metadata'):\n",
    "                            metadata.update(documents[0].metadata)\n",
    "                        metadata['encoding'] = encoding\n",
    "                        extraction_method = 'text_loader'\n",
    "                        logger.info(f\"Successfully extracted text with encoding {encoding}: {len(content)} characters\")\n",
    "                        break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                                logger.warning(f\"Error reading with {encoding} encoding: {str(e)}\")\n",
    "                \n",
    "                # If all encodings fail, try with errors='replace'\n",
    "                if not content or not content.strip():\n",
    "                    try:\n",
    "                        loader = TextLoader(file_path, encoding='utf-8', autodetect_encoding=True)\n",
    "                        documents = loader.load()\n",
    "                        content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                        if documents and hasattr(documents[0], 'metadata'):\n",
    "                            metadata.update(documents[0].metadata)\n",
    "                        metadata['encoding'] = 'utf-8 with replacement'\n",
    "                        extraction_method = 'text_loader_with_replacement'\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"TextLoader with replacement failed: {str(e)}\")\n",
    "                        \n",
    "            elif file_extension in ['.csv', '.tsv']:\n",
    "                # Process CSV/TSV files\n",
    "                logger.info(f\"Using CSVLoader for: {file_name}\")\n",
    "                delimiter = ',' if file_extension == '.csv' else '\\t'\n",
    "                loader = CSVLoader(file_path, csv_args={'delimiter': delimiter})\n",
    "                documents = loader.load()\n",
    "                content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                if documents and hasattr(documents[0], 'metadata'):\n",
    "                    metadata.update(documents[0].metadata)\n",
    "                extraction_method = 'csv_loader'\n",
    "                    \n",
    "            elif file_extension == '.eml':\n",
    "                # Process email files\n",
    "                logger.info(f\"Using UnstructuredEmailLoader for: {file_name}\")\n",
    "                loader = UnstructuredEmailLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                if documents and hasattr(documents[0], 'metadata'):\n",
    "                    metadata.update(documents[0].metadata)\n",
    "                extraction_method = 'unstructured_email'\n",
    "        \n",
    "            else:\n",
    "                # For other file types, try using TextLoader as a fallback\n",
    "                logger.info(f\"Using TextLoader as fallback for: {file_name}\")\n",
    "                try:\n",
    "                    loader = TextLoader(file_path, encoding='utf-8', autodetect_encoding=True)\n",
    "                    documents = loader.load()\n",
    "                    content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "                    if documents and hasattr(documents[0], 'metadata'):\n",
    "                        metadata.update(documents[0].metadata)\n",
    "                    extraction_method = 'text_loader_fallback'\n",
    "                except Exception as e:\n",
    "                        logger.warning(f\"TextLoader fallback failed: {str(e)}\")\n",
    "                \n",
    "            # Verify we got content\n",
    "            if not content or not content.strip():\n",
    "                logger.warning(f\"No content extracted from {file_name}\")\n",
    "                return {\n",
    "                    'content': f\"Could not extract content from {file_name}\",\n",
    "                    'metadata': metadata,\n",
    "                    'extraction_method': 'failed'\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Successfully extracted content with {extraction_method}: {len(content)} characters\")\n",
    "            return {\n",
    "                'content': content,\n",
    "                'metadata': metadata,\n",
    "                'extraction_method': extraction_method\n",
    "                    }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing document {file_name}: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            return {\n",
    "                'content': f\"Error processing document: {str(e)}\",\n",
    "                'metadata': {'error': str(e), 'file_path': file_path},\n",
    "                'extraction_method': 'error'\n",
    "            }\n",
    "            \n",
    "    def _process_audio(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process audio file and generate transcription with better error handling.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to audio file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processed audio data\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing audio file: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Check if file exists and is accessible\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"Audio file not found: {file_path}\")\n",
    "            return {}\n",
    "            \n",
    "        if not os.access(file_path, os.R_OK):\n",
    "            logger.error(f\"Cannot read audio file (permission denied): {file_path}\")\n",
    "            return {}\n",
    "            \n",
    "        try:\n",
    "            # Get file information\n",
    "            file_info = os.stat(file_path)\n",
    "            file_size_mb = file_info.st_size / (1024 * 1024)\n",
    "            logger.info(f\"Audio file size: {file_size_mb:.2f}MB\")\n",
    "            \n",
    "            # Skip excessively large files - they likely will cause problems\n",
    "            if file_size_mb > 500:  # 500MB\n",
    "                logger.warning(f\"Audio file too large ({file_size_mb:.2f}MB), skipping: {os.path.basename(file_path)}\")\n",
    "                return {}\n",
    "                \n",
    "            # Determine transcription settings based on file size\n",
    "            use_conservative_settings = False\n",
    "            transcription_timeout = 300  # Default 5 minutes\n",
    "            \n",
    "            # Adjust settings for large files\n",
    "            if file_size_mb > 50:  # Files larger than 50MB\n",
    "                logger.info(f\"Large audio file detected ({file_size_mb:.2f}MB), using optimized processing\")\n",
    "                use_conservative_settings = True\n",
    "                transcription_timeout = 600  # 10 minutes for large files\n",
    "            \n",
    "            # Ensure Milvus is running before transcription (which is memory-intensive)\n",
    "            self.ensure_milvus_running()\n",
    "            \n",
    "            # Try to transcribe the audio with retry logic and timeouts\n",
    "            max_retries = 2\n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    transcription = self.transcribe_audio(\n",
    "                        file_path,\n",
    "                        use_conservative_settings=use_conservative_settings,\n",
    "                        timeout=transcription_timeout\n",
    "                    )\n",
    "                    return {\n",
    "                        \"path\": file_path,\n",
    "                        \"transcription\": transcription,\n",
    "                        \"duration\": file_info.st_size / 16000  # Rough estimate of duration in seconds\n",
    "                    }\n",
    "                except RuntimeError as e:\n",
    "                    if \"timed out\" in str(e) and attempt < max_retries:\n",
    "                        logger.warning(f\"Transcription attempt {attempt+1}/{max_retries+1} timed out, retrying...\")\n",
    "                        # Ensure Milvus is still running before retry\n",
    "                        self.ensure_milvus_running()\n",
    "                        continue\n",
    "                    else:\n",
    "                        logger.error(f\"Failed to transcribe audio after {attempt+1} attempts: {str(e)}\")\n",
    "                        return {}\n",
    "        except Exception as e:\n",
    "                    logger.error(f\"Unexpected error processing audio: {str(e)}\")\n",
    "                    return {}\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing audio file {os.path.basename(file_path)}: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _process_image(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process image file metadata.\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        return {\n",
    "            'content': f\"Image file: {file_name}\",\n",
    "            'metadata': {'file_path': file_path, 'file_type': 'image'},\n",
    "            'extraction_method': 'image_metadata'\n",
    "        }\n",
    "        \n",
    "    def _process_video_metadata(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process video file metadata.\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        return {\n",
    "            'content': f\"Video file: {file_name}\",\n",
    "            'metadata': {'file_path': file_path, 'file_type': 'video'},\n",
    "            'extraction_method': 'video_metadata'\n",
    "        }\n",
    "\n",
    "    def extract_video_frames(self, video_path: str, sample_rate: int = 1) -> List[np.ndarray]:\n",
    "        \"\"\"Extract frames from video using OpenCV.\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to the video file\n",
    "            sample_rate: Interval between frames to extract (1 = every frame, 30 = one frame per second at 30fps)\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted frames as numpy arrays\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting frames from video: {os.path.basename(video_path)}\")\n",
    "        frames = []\n",
    "        \n",
    "        # Check if file exists and is readable\n",
    "        if not os.path.exists(video_path):\n",
    "            logger.error(f\"Video file not found: {video_path}\")\n",
    "            return frames\n",
    "            \n",
    "        # Open video directly with OpenCV - no need for subprocess\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        if not video.isOpened():\n",
    "            logger.error(f\"Failed to open video: {video_path}\")\n",
    "            return frames\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count_total = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count_total / fps if fps > 0 else 0\n",
    "        \n",
    "        # Determine if this is likely a presentation or lecture based on the filename or other heuristics\n",
    "        file_name = os.path.basename(video_path).lower()\n",
    "        is_presentation = any(keyword in file_name for keyword in [\"presentation\", \"lecture\", \"ppt\", \"slide\", \"class\", \"course\", \"workshop\"])\n",
    "        \n",
    "        # Set appropriate sample rate based on video type and length\n",
    "        # For presentations: ~1 frame every 5-10 seconds is usually sufficient\n",
    "        if is_presentation:\n",
    "            # For presentations, we want fewer frames (1 frame per ~5 seconds)\n",
    "            effective_sample_rate = max(int(fps * 5), sample_rate)\n",
    "            logger.info(f\"Video appears to be a presentation/lecture. Using higher sample rate: 1 frame every ~5 seconds\")\n",
    "        elif duration > 600:  # For videos longer than 10 minutes\n",
    "            # For long videos, we want ~3 frames per minute\n",
    "            effective_sample_rate = max(int(fps * 20), sample_rate)\n",
    "            logger.info(f\"Long video detected ({duration:.1f} seconds). Using higher sample rate: {effective_sample_rate}\")\n",
    "        else:\n",
    "            # For regular, shorter videos, use the provided sample rate\n",
    "            effective_sample_rate = sample_rate\n",
    "            logger.info(f\"Using standard sample rate: {effective_sample_rate}\")\n",
    "        \n",
    "        # Set a maximum limit on frames to extract to avoid memory issues\n",
    "        max_frames_to_extract = 30  # Maximum frames to extract\n",
    "        \n",
    "        # Process the video in a memory-efficient way\n",
    "        frame_count = 0\n",
    "        extracted_count = 0\n",
    "        \n",
    "        while video.isOpened() and extracted_count < max_frames_to_extract:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % effective_sample_rate == 0:\n",
    "                # Convert frame from BGR to RGB format (standard for image models)\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(rgb_frame)\n",
    "                extracted_count += 1\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Add a maximum frame count safety check\n",
    "            if frame_count > 10000:  # Arbitrary limit to prevent processing extremely large videos\n",
    "                logger.warning(f\"Reached maximum frame count limit for video: {os.path.basename(video_path)}\")\n",
    "                break\n",
    "            \n",
    "        video.release()\n",
    "        \n",
    "        logger.info(f\"Extracted {len(frames)} frames from video (total frames: {frame_count}, duration: {duration:.1f}s)\")\n",
    "        return frames\n",
    "\n",
    "    def transcribe_audio(self, audio_path: str, use_conservative_settings: bool = False, timeout: int = 300) -> str:\n",
    "        \"\"\"Transcribe audio using Whisper.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to the audio file\n",
    "            use_conservative_settings: Whether to use more conservative settings (slower but more accurate)\n",
    "            timeout: Maximum time to allow for transcription in seconds\n",
    "            \n",
    "        Returns:\n",
    "            Transcription text\n",
    "        \"\"\"\n",
    "        logger.info(f\"Transcribing audio: {os.path.basename(audio_path)}\")\n",
    "        \n",
    "        # Use the already loaded whisper model directly (loaded in _init_embedding_models)\n",
    "        # This avoids creating new processes and the associated fork warnings\n",
    "        try:\n",
    "            # If the audio is long, use a more efficient approach\n",
    "            file_size = os.path.getsize(audio_path) / (1024 * 1024)  # Size in MB\n",
    "            \n",
    "            # Load directly without creating a new process\n",
    "            # Use a more memory-efficient approach for very large files\n",
    "            if file_size > 100 or use_conservative_settings:  # For files > 100MB\n",
    "                logger.info(f\"Large audio file detected ({file_size:.1f}MB), using conservative settings\")\n",
    "                result = self.whisper_model.transcribe(\n",
    "                    audio_path,\n",
    "                    fp16=False,  # Use FP32 for better stability\n",
    "                    language=\"en\",  # Set language explicitly\n",
    "                    initial_prompt=\"The following is a transcript of audio content\"\n",
    "                )\n",
    "            else:\n",
    "                # Standard approach for regular files\n",
    "                result = self.whisper_model.transcribe(audio_path)\n",
    "                \n",
    "                # Check for empty result\n",
    "                if not result or \"text\" not in result or not result[\"text\"].strip():\n",
    "                    logger.warning(f\"Whisper model produced empty transcription for {os.path.basename(audio_path)}\")\n",
    "                    return \"\"\n",
    "                \n",
    "                return result[\"text\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error transcribing audio: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    # ----- EMBEDDING GENERATION METHODS -----\n",
    "    \n",
    "    def generate_embedding(self, content: Any, embedding_type: EmbeddingType) -> np.ndarray:\n",
    "        \"\"\"Generate embedding based on the specified type.\n",
    "        \n",
    "        Supports:\n",
    "        - CLIP_TEXT: For text content\n",
    "        - CLIP_IMAGE: For images and video frames\n",
    "        - WAV2VEC: For audio content\n",
    "        \"\"\"\n",
    "        logger.info(f\"Generating {embedding_type.value} embedding\")\n",
    "        \n",
    "        if embedding_type == EmbeddingType.CLIP_TEXT:\n",
    "            return self._generate_text_embedding(content)\n",
    "        elif embedding_type == EmbeddingType.CLIP_IMAGE:\n",
    "            return self._generate_image_embedding(content)\n",
    "        elif embedding_type == EmbeddingType.WAV2VEC:\n",
    "            return self._generate_audio_embedding(content)\n",
    "        \n",
    "        raise ValueError(f\"Unsupported embedding type: {embedding_type}\")\n",
    "    \n",
    "    def _generate_text_embedding(self, content: str) -> np.ndarray:\n",
    "        \"\"\"Generate text embedding using CLIP.\"\"\"\n",
    "        if not isinstance(content, str):\n",
    "            raise ValueError(\"Content must be a string for CLIP_TEXT embedding type\")\n",
    "            \n",
    "        # Split text into chunks of approximately 77 tokens\n",
    "        chunks = [content[i:i+77] for i in range(0, len(content), 77)]\n",
    "        chunk_embeddings = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            inputs = self.clip_processor(text=chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "            with torch.no_grad():\n",
    "                text_features = self.clip_model.get_text_features(**inputs)\n",
    "            chunk_embeddings.append(text_features.detach().numpy())\n",
    "        \n",
    "        # Average the embeddings from all chunks\n",
    "        return np.mean(chunk_embeddings, axis=0)\n",
    "    \n",
    "    def _generate_image_embedding(self, content: Any) -> np.ndarray:\n",
    "        \"\"\"Generate image embedding using CLIP.\"\"\"\n",
    "        if isinstance(content, str):  # If content is a file path\n",
    "            image = Image.open(content)\n",
    "        else:  # If content is already a PIL Image or numpy array\n",
    "            image = Image.fromarray(content) if isinstance(content, np.ndarray) else content\n",
    "        \n",
    "        inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.get_image_features(**inputs)\n",
    "        return image_features.detach().numpy()\n",
    "    \n",
    "    def _generate_audio_embedding(self, file_path_or_content, is_file_path=True):\n",
    "        \"\"\"\n",
    "        Generate audio embedding from file path or content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If it's a file path, check if it exists\n",
    "            if is_file_path:\n",
    "                if not os.path.exists(file_path_or_content):\n",
    "                    logger.error(f\"Audio file doesn't exist: {file_path_or_content}\")\n",
    "                    return None\n",
    "                \n",
    "                # Load audio for embedding\n",
    "                try:\n",
    "                    # Use librosa to load audio file\n",
    "                    audio, _ = librosa.load(file_path_or_content, sr=16000, mono=True)\n",
    "                    \n",
    "                    # Generate embedding using Wav2Vec2\n",
    "                    inputs = self.wav2vec_processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.wav2vec_model(inputs).last_hidden_state\n",
    "                    \n",
    "                    # Average the embeddings from all frames\n",
    "                    embedding = outputs.mean(dim=1).squeeze().numpy()\n",
    "                    \n",
    "                    # Resize to 512 dimensions to match Milvus schema\n",
    "                    if embedding.shape[0] != 512:\n",
    "                        # Use PCA to reduce dimensions if needed, or simple resizing for now\n",
    "                        # For simplicity, we'll use the first 512 dimensions if > 512\n",
    "                        # or pad with zeros if < 512\n",
    "                        if embedding.shape[0] > 512:\n",
    "                            embedding = embedding[:512]\n",
    "                        else:\n",
    "                            padding = np.zeros(512 - embedding.shape[0])\n",
    "                            embedding = np.concatenate((embedding, padding))\n",
    "                    \n",
    "                    return embedding\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing audio file: {str(e)}\")\n",
    "                    return None\n",
    "            else:\n",
    "                # For non-file-path content (like transcription text), use CLIP instead\n",
    "                # This ensures consistency with the Milvus collection schema\n",
    "                text_embedding = self.generate_embedding(file_path_or_content, EmbeddingType.CLIP_TEXT)\n",
    "                return text_embedding\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating audio embedding: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # ----- VECTOR DATABASE METHODS -----\n",
    "\n",
    "    def setup_milvus(self):\n",
    "        \"\"\"Setup Milvus connection and collection.\"\"\"\n",
    "        try:\n",
    "            # Check docker is running\n",
    "            logger.info(\"Checking if Docker is running...\")\n",
    "            docker_check = subprocess.run(['docker', 'info'], capture_output=True, text=True)\n",
    "            if docker_check.returncode != 0:\n",
    "                logger.error(\"Docker is not running. Please start Docker first.\")\n",
    "                raise Exception(\"Docker is not running\")\n",
    "            \n",
    "            # Check if milvus network exists, create if needed\n",
    "            logger.info(\"Checking for milvus network...\")\n",
    "            network_check = subprocess.run(['docker', 'network', 'ls', '--filter', 'name=milvus', '--format', '{{.Name}}'],\n",
    "                                          capture_output=True, text=True)\n",
    "            if 'milvus' not in network_check.stdout:\n",
    "                logger.info(\"Creating milvus network...\")\n",
    "                subprocess.run(['docker', 'network', 'create', 'milvus'], check=True)\n",
    "                logger.info(\"Created milvus network\")\n",
    "            \n",
    "            # Use docker-compose to manage Milvus and supporting services\n",
    "            logger.info(\"Starting Milvus and supporting services using docker-compose...\")\n",
    "            \n",
    "            # Get the directory of the current script to find docker-compose.yml\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            compose_file = os.path.join(current_dir, 'docker-compose.yml')\n",
    "            \n",
    "            # Check if docker-compose.yml exists\n",
    "            if not os.path.exists(compose_file):\n",
    "                logger.warning(f\"docker-compose.yml not found at {compose_file}\")\n",
    "                compose_file = 'docker-compose.yml'  # Try in current working directory\n",
    "            \n",
    "            # Start services with docker-compose\n",
    "            subprocess.run(['docker-compose', '-f', compose_file, 'up', '-d'], check=True)\n",
    "            logger.info(\"Started Milvus and supporting services\")\n",
    "            \n",
    "            # Wait for Milvus to be ready\n",
    "            logger.info(\"Waiting for Milvus to be ready...\")\n",
    "            max_retries = 45  # Increased from 30 to allow more startup time\n",
    "            retry_interval = 3  # Increased from 2 seconds to give more time between retries\n",
    "            \n",
    "            for i in range(max_retries):\n",
    "                try:\n",
    "                    # Try to connect with a timeout\n",
    "                    connections.connect(host='localhost', port='19530', timeout=10)  # Increased timeout\n",
    "                    logger.info(f\"Milvus is ready after {i * retry_interval} seconds\")\n",
    "                    # Disconnect to reconnect properly later\n",
    "                    connections.disconnect(\"default\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if i < max_retries - 1:\n",
    "                        logger.info(f\"Waiting for Milvus to be ready... Attempt {i+1}/{max_retries}\")\n",
    "                        logger.debug(f\"Connection error: {str(e)}\")\n",
    "                        time.sleep(retry_interval)\n",
    "                    else:\n",
    "                        logger.error(\"Timed out waiting for Milvus to be ready\")\n",
    "                        logger.error(f\"Last error: {str(e)}\")\n",
    "                        raise Exception(\"Timed out waiting for Milvus to be ready\")\n",
    "            \n",
    "            # Connect to Milvus\n",
    "            logger.info(\"Connecting to Milvus server on localhost:19530...\")\n",
    "            connections.connect(host='localhost', port='19530')\n",
    "            \n",
    "            collection_name = \"personal_rag\"\n",
    "            \n",
    "            # Define schema with timestamp field\n",
    "            fields = [\n",
    "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "                FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=512),\n",
    "                FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=64),\n",
    "                FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=256),\n",
    "                FieldSchema(name=\"timestamp\", dtype=DataType.INT64)\n",
    "            ]\n",
    "            schema = CollectionSchema(fields=fields, description=\"Personal RAG collection\")\n",
    "            \n",
    "            # Check if collection exists\n",
    "            should_create_collection = True\n",
    "            if utility.has_collection(collection_name):\n",
    "                logger.info(f\"Collection '{collection_name}' already exists\")\n",
    "                # Use existing collection instead of dropping and recreating\n",
    "                self.collection = Collection(collection_name)\n",
    "                \n",
    "                # Try to load the collection with retry logic\n",
    "                load_retries = 3\n",
    "                load_retry_interval = 2\n",
    "                \n",
    "                for load_attempt in range(load_retries):\n",
    "                    try:\n",
    "                        self.collection.load()\n",
    "                        logger.info(f\"Successfully loaded existing collection '{collection_name}'\")\n",
    "                        should_create_collection = False\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if load_attempt < load_retries - 1:\n",
    "                            logger.warning(f\"Failed to load collection (attempt {load_attempt+1}/{load_retries}): {str(e)}\")\n",
    "                            logger.warning(f\"Retrying in {load_retry_interval} seconds...\")\n",
    "                            time.sleep(load_retry_interval)\n",
    "                            load_retry_interval *= 2  # Exponential backoff\n",
    "                        else:\n",
    "                            logger.error(f\"Failed to load collection after {load_retries} attempts\")\n",
    "                            raise\n",
    "            \n",
    "            # Create collection only if it doesn't exist\n",
    "            if should_create_collection:\n",
    "                logger.info(f\"Creating new Milvus collection '{collection_name}' with timestamp field\")\n",
    "                self.collection = Collection(name=collection_name, schema=schema)\n",
    "                # Create HNSW index with more balanced parameters for stability\n",
    "                index_params = {\n",
    "                    \"metric_type\": \"L2\",\n",
    "                    \"index_type\": \"HNSW\",\n",
    "                    \"params\": {\n",
    "                        \"M\": 12,  # Reduced from 16 to reduce memory usage and improve stability\n",
    "                        \"efConstruction\": 300,  # Reduced from 500 for better stability\n",
    "                        \"ef\": 100  # Add search quality parameter\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Create index with retry logic\n",
    "                create_index_retries = 3\n",
    "                create_index_retry_interval = 2\n",
    "                \n",
    "                for index_attempt in range(create_index_retries):\n",
    "                    try:\n",
    "                        self.collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "                        logger.info(f\"Successfully created index on collection '{collection_name}'\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if index_attempt < create_index_retries - 1:\n",
    "                            logger.warning(f\"Failed to create index (attempt {index_attempt+1}/{create_index_retries}): {str(e)}\")\n",
    "                            logger.warning(f\"Retrying in {create_index_retry_interval} seconds...\")\n",
    "                            time.sleep(create_index_retry_interval)\n",
    "                            create_index_retry_interval *= 2  # Exponential backoff\n",
    "                        else:\n",
    "                            logger.error(f\"Failed to create index after {create_index_retries} attempts\")\n",
    "                            raise\n",
    "                \n",
    "                # Load collection with retry logic\n",
    "                load_retries = 3\n",
    "                load_retry_interval = 2\n",
    "                \n",
    "                for load_attempt in range(load_retries):\n",
    "                    try:\n",
    "                        self.collection.load()\n",
    "                        logger.info(f\"Successfully loaded new collection '{collection_name}'\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if load_attempt < load_retries - 1:\n",
    "                            logger.warning(f\"Failed to load new collection (attempt {load_attempt+1}/{load_retries}): {str(e)}\")\n",
    "                            logger.warning(f\"Retrying in {load_retry_interval} seconds...\")\n",
    "                            time.sleep(load_retry_interval)\n",
    "                            load_retry_interval *= 2  # Exponential backoff\n",
    "                        else:\n",
    "                            logger.error(f\"Failed to load new collection after {load_retries} attempts\")\n",
    "                            raise\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup Milvus: {str(e)}\")\n",
    "            logger.error(\"Make sure Docker is running on your system\")\n",
    "            raise\n",
    "    \n",
    "    def reset_milvus(self):\n",
    "        \"\"\"Reset the Milvus database by dropping and recreating the collection.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Attempting to drop collection 'personal_rag'...\")\n",
    "            # First check if collection exists\n",
    "            if utility.has_collection(\"personal_rag\"):\n",
    "                utility.drop_collection(\"personal_rag\")\n",
    "                logger.info(\"Collection 'personal_rag' has been dropped\")\n",
    "            else:\n",
    "                logger.info(\"Collection 'personal_rag' does not exist, nothing to drop\")\n",
    "            \n",
    "            # Recreate the collection with proper schema\n",
    "            self.setup_collection()\n",
    "            logger.info(\"Collection 'personal_rag' has been recreated with fresh schema\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to reset Milvus: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_collection(self):\n",
    "        \"\"\"Set up the Milvus collection with proper schema.\"\"\"\n",
    "        collection_name = \"personal_rag\"\n",
    "        \n",
    "        # Define fields for the collection\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=100),\n",
    "            FieldSchema(name=\"file_path\", dtype=DataType.VARCHAR, max_length=500),\n",
    "            FieldSchema(name=\"chunk_index\", dtype=DataType.INT64),\n",
    "            FieldSchema(name=\"last_modified\", dtype=DataType.INT64),\n",
    "            FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "            FieldSchema(name=\"metadata\", dtype=DataType.JSON),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1536)\n",
    "        ]\n",
    "        \n",
    "        # Create collection schema\n",
    "        schema = CollectionSchema(fields=fields, description=\"Personal RAG data\")\n",
    "        \n",
    "        # Create collection\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        \n",
    "        # Create an index for the embedding field\n",
    "        index_params = {\n",
    "            \"index_type\": \"IVF_FLAT\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"nlist\": 1024}\n",
    "        }\n",
    "        collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "        \n",
    "        logger.info(f\"Created Milvus collection '{collection_name}' with proper schema and index\")\n",
    "        return collection\n",
    "\n",
    "    def add_to_milvus(self, content: str, embedding: np.ndarray, doc_type: str, source: str, timestamp: int = None):\n",
    "        \"\"\"Add document to Milvus.\"\"\"\n",
    "        # Ensure embedding is a 1D array and convert to list\n",
    "        if embedding.ndim > 1:\n",
    "            embedding = embedding.squeeze()\n",
    "        embedding_list = embedding.tolist()\n",
    "        \n",
    "        # Use current timestamp if none provided\n",
    "        if timestamp is None:\n",
    "            timestamp = int(time.time())\n",
    "        \n",
    "        # Ensure connection is available\n",
    "        self._ensure_milvus_connection()\n",
    "        \n",
    "        # Add retry mechanism for Milvus operations\n",
    "        max_retries = 5\n",
    "        retry_delay = 2  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.collection.insert([\n",
    "                    {\"content\": content, \"embedding\": embedding_list, \"type\": doc_type, \"source\": source, \"timestamp\": timestamp}\n",
    "                ])\n",
    "                logger.info(f\"Added document to Milvus: type={doc_type}, source={source}\")\n",
    "                return  # Success, exit function\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.warning(f\"Milvus insertion failed (attempt {attempt+1}/{max_retries}): {str(e)}\")\n",
    "                    logger.warning(f\"Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    # Increase delay for next attempt (exponential backoff)\n",
    "                    retry_delay *= 2\n",
    "                    \n",
    "                    # Try to reconnect to Milvus if the error appears to be connection-related\n",
    "                    if \"connect\" in str(e).lower() or \"connection\" in str(e).lower():\n",
    "                        try:\n",
    "                            logger.info(\"Attempting to reconnect to Milvus...\")\n",
    "                            connections.disconnect(\"default\")\n",
    "                            time.sleep(1)\n",
    "                            connections.connect(host='localhost', port='19530')\n",
    "                            logger.info(\"Reconnected to Milvus\")\n",
    "                        except Exception as reconnect_error:\n",
    "                            logger.warning(f\"Failed to reconnect to Milvus: {str(reconnect_error)}\")\n",
    "                else:\n",
    "                    logger.error(f\"Failed to add document to Milvus after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "\n",
    "    def add_to_milvus_with_timestamp(self, content, embedding, doc_type, source, timestamp):\n",
    "        \"\"\"Add document to Milvus with timestamp.\"\"\"\n",
    "        if embedding.ndim > 1:\n",
    "            embedding = embedding.squeeze()\n",
    "        embedding_list = embedding.tolist()\n",
    "        \n",
    "        # Add retry mechanism for Milvus operations\n",
    "        max_retries = 5\n",
    "        retry_delay = 2  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # First try to find existing document with the same source\n",
    "                search_params = {\"expr\": f'source == \"{source}\"'}\n",
    "                results = self.collection.query(\n",
    "                    expr=search_params[\"expr\"],\n",
    "                    output_fields=[\"id\"]\n",
    "                )\n",
    "                \n",
    "                if results:\n",
    "                    # Update existing document\n",
    "                    self.collection.delete(f'source == \"{source}\"')\n",
    "                \n",
    "                # Insert new or updated document\n",
    "                self.collection.insert([\n",
    "                    {\"content\": content, \"embedding\": embedding_list, \n",
    "                     \"type\": doc_type, \"source\": source, \"timestamp\": timestamp}\n",
    "                ])\n",
    "                logger.info(f\"Added/updated document in Milvus: type={doc_type}, source={source}\")\n",
    "                return  # Success, exit function\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.warning(f\"Milvus operation failed (attempt {attempt+1}/{max_retries}): {str(e)}\")\n",
    "                    logger.warning(f\"Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    # Increase delay for next attempt (exponential backoff)\n",
    "                    retry_delay *= 2\n",
    "                    \n",
    "                    # Try to reconnect to Milvus if the error appears to be connection-related\n",
    "                    if \"connect\" in str(e).lower() or \"connection\" in str(e).lower():\n",
    "                        try:\n",
    "                            logger.info(\"Attempting to reconnect to Milvus...\")\n",
    "                            connections.disconnect(\"default\")\n",
    "                            time.sleep(1)\n",
    "                            connections.connect(host='localhost', port='19530')\n",
    "                            logger.info(\"Reconnected to Milvus\")\n",
    "                        except Exception as reconnect_error:\n",
    "                            logger.warning(f\"Failed to reconnect to Milvus: {str(reconnect_error)}\")\n",
    "                else:\n",
    "                    logger.error(f\"Failed to add/update document in Milvus after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "\n",
    "    # ----- MAIN INGESTION PIPELINE METHODS -----\n",
    "\n",
    "    def process_directory(self, directory_path: str) -> int:\n",
    "        \"\"\"Process all supported files in a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to directory to process\n",
    "            \n",
    "        Returns:\n",
    "            Number of files successfully processed\n",
    "        \"\"\"\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        new_files_count = 0\n",
    "        modified_files_count = 0\n",
    "        logger.info(f\"Processing directory: {directory_path}\")\n",
    "        \n",
    "        # First collect all eligible files to process\n",
    "        files_to_check = []\n",
    "        \n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_extension = os.path.splitext(file_path)[1].lower()\n",
    "                \n",
    "                # Check if file type is supported\n",
    "                is_supported, _ = self.config.is_supported(file_extension, \"local\")\n",
    "                if not is_supported:\n",
    "                    continue\n",
    "                \n",
    "                # Get file info for processing\n",
    "                file_timestamp = int(os.path.getmtime(file_path))\n",
    "                files_to_check.append((file_path, file_timestamp))\n",
    "        \n",
    "        # Log the number of files we'll be checking\n",
    "        logger.info(f\"Found {len(files_to_check)} supported files to check in {directory_path}\")\n",
    "        \n",
    "        # Process files in smaller batches to avoid overloading Milvus\n",
    "        batch_size = 10\n",
    "        for i in range(0, len(files_to_check), batch_size):\n",
    "            batch = files_to_check[i:i+batch_size]\n",
    "            logger.info(f\"Processing batch {i//batch_size + 1}/{(len(files_to_check) + batch_size - 1)//batch_size}\")\n",
    "            \n",
    "            # Ensure Milvus is running before each batch\n",
    "            self.ensure_milvus_running()\n",
    "            \n",
    "            for file_path, file_timestamp in batch:\n",
    "                # Check if file needs updating\n",
    "                is_new = self._is_new_file(file_path)\n",
    "                if is_new:\n",
    "                    logger.info(f\"Found NEW file to process: {file_path}\")\n",
    "                    new_files_count += 1\n",
    "                \n",
    "                result = self.process_and_ingest_file(file_path, file_timestamp)\n",
    "                if result:\n",
    "                    processed_count += 1\n",
    "                    if not is_new:\n",
    "                        modified_files_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "        \n",
    "        logger.info(f\"Directory processing summary: {processed_count} files processed ({new_files_count} new, {modified_files_count} modified), {skipped_count} skipped from {directory_path}\")\n",
    "        return processed_count\n",
    "\n",
    "    def process_and_ingest_file(self, file_path: str, file_timestamp: int = None) -> bool:\n",
    "        \"\"\"Process and ingest a file into the vector database.\n",
    "        \n",
    "        This is the main pipeline that:\n",
    "        1. Processes the file based on its type (Tika/OpenCV/Whisper)\n",
    "        2. Generates appropriate embeddings (CLIP/Wav2Vec)\n",
    "        3. Stores in Milvus\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file to process\n",
    "            file_timestamp: Optional pre-computed file timestamp\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success or failure\n",
    "        \"\"\"\n",
    "        try:\n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            # Check if file type is supported\n",
    "            is_supported, doc_type = self.config.is_supported(file_extension, \"local\")\n",
    "            if not is_supported:\n",
    "                logger.warning(f\"Unsupported file type: {file_extension}\")\n",
    "                return False\n",
    "\n",
    "            # Get file modification time as timestamp if not provided\n",
    "            if file_timestamp is None:\n",
    "                file_timestamp = int(os.path.getmtime(file_path))\n",
    "            source_path = file_path  # Use file path as source\n",
    "            \n",
    "            # Check if file needs to be updated\n",
    "            needs_update = self._check_if_file_needs_update(source_path, file_timestamp)\n",
    "            if not needs_update:\n",
    "                logger.info(f\"Skipping unchanged file: {file_path}\")\n",
    "                return False\n",
    "            \n",
    "            # Pre-check Milvus container status\n",
    "            self.ensure_milvus_running()\n",
    "            \n",
    "            doc_config = self.config.sources[\"local\"].document_types[doc_type]\n",
    "            logger.info(f\"Processing {file_name} as {doc_type}\")\n",
    "            \n",
    "            # Track processing start time for timeout detection\n",
    "            start_time = time.time()\n",
    "            timeout_seconds = 300  # 5 minutes timeout for processing a single file\n",
    "            \n",
    "            # For larger file types that might cause Milvus issues, add monitoring\n",
    "            large_file_types = [\"video_frame\", \"audio\"]\n",
    "            should_monitor = doc_type in large_file_types\n",
    "            \n",
    "            if should_monitor:\n",
    "                logger.info(f\"Enabling Milvus monitoring for {doc_type} processing\")\n",
    "            \n",
    "            # Step 1: Process document based on type\n",
    "            if doc_type == \"document\":\n",
    "                # Use Tika to parse document\n",
    "                parsed_doc = self.process_document(file_path)\n",
    "                \n",
    "                # Add container status check for long-running tasks\n",
    "                if should_monitor and (time.time() - start_time) > 60:  # Check after 1 minute\n",
    "                    logger.info(\"Checking Milvus container status during document processing...\")\n",
    "                    self.ensure_milvus_running()\n",
    "                \n",
    "                if not parsed_doc or not parsed_doc.get('content'):\n",
    "                    logger.warning(f\"Failed to extract content from document: {file_path}\")\n",
    "                    return False\n",
    "                \n",
    "                # Step 2: Generate embedding\n",
    "                content = parsed_doc['content']\n",
    "                embedding = self.generate_embedding(content, doc_config.embedding_type)\n",
    "                \n",
    "                # Step 3: Add to Milvus\n",
    "                self.add_to_milvus(content, embedding, doc_type, source_path, file_timestamp)\n",
    "                return True\n",
    "                \n",
    "            elif doc_type == \"image\":\n",
    "                # Process image\n",
    "                parsed_image = self._process_image(file_path)\n",
    "                \n",
    "                # Add container status check for long-running tasks\n",
    "                if should_monitor and (time.time() - start_time) > 60:  # Check after 1 minute\n",
    "                    logger.info(\"Checking Milvus container status during image processing...\")\n",
    "                    self.ensure_milvus_running()\n",
    "                \n",
    "                if not parsed_image or not parsed_image.get('image'):\n",
    "                    logger.warning(f\"Failed to process image: {file_path}\")\n",
    "                    return False\n",
    "                \n",
    "                # Generate embedding from image\n",
    "                image = parsed_image['image']\n",
    "                embedding = self.generate_embedding(image, doc_config.embedding_type)\n",
    "                \n",
    "                # Add image description as content\n",
    "                content = f\"Image: {file_name}\\nPath: {file_path}\\nDescription: {parsed_image.get('description', 'No description')}\"\n",
    "                \n",
    "                # Add to Milvus\n",
    "                self.add_to_milvus(content, embedding, doc_type, source_path, file_timestamp)\n",
    "                return True\n",
    "                \n",
    "            elif doc_type == \"audio\":\n",
    "                # Process audio file\n",
    "                parsed_audio = self._process_audio(file_path)\n",
    "                \n",
    "                # Periodically check container status for long audio files\n",
    "                if should_monitor:\n",
    "                    # Check container status every minute during audio processing\n",
    "                    check_interval = 60  # seconds\n",
    "                    last_check_time = start_time\n",
    "                    \n",
    "                    while (time.time() - start_time) < timeout_seconds:\n",
    "                        if (time.time() - last_check_time) > check_interval:\n",
    "                            logger.info(\"Checking Milvus container status during audio processing...\")\n",
    "                            if not self.ensure_milvus_running():\n",
    "                                logger.warning(\"Milvus container issue detected and fixed during audio processing\")\n",
    "                            last_check_time = time.time()\n",
    "                        \n",
    "                        # Sleep briefly to avoid CPU spinning\n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                        # Check if processing completed\n",
    "                        if parsed_audio and parsed_audio.get('transcription'):\n",
    "                            break\n",
    "                \n",
    "                if not parsed_audio or not parsed_audio.get('transcription'):\n",
    "                    logger.warning(f\"Failed to process audio: {file_path}\")\n",
    "                    return False\n",
    "                \n",
    "                # Generate embedding from audio transcription\n",
    "                # IMPORTANT: Always use CLIP_TEXT for transcriptions regardless of doc_config\n",
    "                # This ensures dimension compatibility with Milvus\n",
    "                transcription = parsed_audio['transcription']\n",
    "                embedding = self.generate_embedding(transcription, EmbeddingType.CLIP_TEXT)\n",
    "                \n",
    "                # Ensure Milvus is running before attempting to store\n",
    "                self.ensure_milvus_running()\n",
    "                \n",
    "                # Add to Milvus\n",
    "                content = f\"Audio: {file_name}\\nPath: {file_path}\\nTranscription: {transcription}\"\n",
    "                self.add_to_milvus(content, embedding, doc_type, source_path, file_timestamp)\n",
    "                return True\n",
    "                \n",
    "            elif doc_type == \"video_frame\":\n",
    "                # For videos, process each frame separately\n",
    "                video_metadata = self._process_video_metadata(file_path)\n",
    "                \n",
    "                # Extract frames\n",
    "                try:\n",
    "                    # Detect if this appears to be a presentation/lecture video\n",
    "                    is_presentation = False\n",
    "                    if 'lecture' in file_name.lower() or 'presentation' in file_name.lower():\n",
    "                        is_presentation = True\n",
    "                        logger.info(f\"Detected presentation/lecture video: {file_name}\")\n",
    "                    \n",
    "                    # Set appropriate sample rate\n",
    "                    sample_rate = 1  # Default: 1 frame per second\n",
    "                    if is_presentation:\n",
    "                        sample_rate = 10  # For presentations, sample less frequently\n",
    "                    \n",
    "                    # First, add original video file metadata to Milvus to mark it as processed\n",
    "                    # This ensures the video itself gets marked as processed, not just its frames\n",
    "                    try:\n",
    "                        # Create a text embedding for the video metadata\n",
    "                        video_info = f\"Video file: {file_name}\\nPath: {file_path}\"\n",
    "                        video_embedding = self.generate_embedding(video_info, EmbeddingType.CLIP_TEXT)\n",
    "                        # Add the original video entry to Milvus with the direct file path as source\n",
    "                        self.add_to_milvus(video_info, video_embedding, \"video\", source_path, file_timestamp)\n",
    "                        logger.info(f\"Added video metadata to Milvus: {file_name}\")\n",
    "                    except Exception as vm_e:\n",
    "                        logger.warning(f\"Could not add video metadata for {file_name}: {str(vm_e)}\")\n",
    "                    \n",
    "                    frames = self.extract_video_frames(file_path, sample_rate)\n",
    "                    logger.info(f\"Extracted {len(frames)} frames from video {file_name}\")\n",
    "                    \n",
    "                    # Process audio from video if available\n",
    "                    try:\n",
    "                        self.extract_and_process_audio_from_video(file_path, file_timestamp)\n",
    "                    except Exception as audio_e:\n",
    "                        logger.warning(f\"Could not extract audio from video {file_name}: {str(audio_e)}\")\n",
    "                    \n",
    "                    # For each frame, generate embedding and add to Milvus\n",
    "                    frame_count = 0\n",
    "                    for i, frame in enumerate(frames):\n",
    "                        try:\n",
    "                            # Periodically check container status\n",
    "                            if should_monitor and i > 0 and i % 10 == 0:  # Check every 10 frames\n",
    "                                logger.info(f\"Checking Milvus container at frame {i}/{len(frames)}...\")\n",
    "                                self.ensure_milvus_running()\n",
    "                            \n",
    "                            # Skip completely black or white frames\n",
    "                            if frame.mean() < 5 or frame.mean() > 250:\n",
    "                                logger.debug(f\"Skipping blank frame {i}\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Generate embedding\n",
    "                            embedding = self.generate_embedding(frame, doc_config.embedding_type)\n",
    "                            \n",
    "                            # Create content description\n",
    "                            content = f\"Video: {file_name}\\nPath: {file_path}\\nFrame: {i}\\nTimestamp: {int(i/sample_rate)} seconds\"\n",
    "                            \n",
    "                            # Create unique source identifier for each frame\n",
    "                            frame_source = f\"{source_path}#frame{i}\"\n",
    "                            \n",
    "                            # Add to Milvus with timestamp\n",
    "                            self.add_to_milvus(content, embedding, doc_type, frame_source, file_timestamp)\n",
    "                            frame_count += 1\n",
    "                            \n",
    "                        except Exception as frame_e:\n",
    "                            logger.warning(f\"Error processing frame {i} from {file_name}: {str(frame_e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    logger.info(f\"Successfully processed {frame_count}/{len(frames)} frames from video {file_name}\")\n",
    "                    return frame_count > 0\n",
    "            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting frames from video {file_path}: {str(e)}\")\n",
    "                    return False\n",
    "\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported document type: {doc_type}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def extract_and_process_audio_from_video(self, video_path: str, timestamp: int) -> bool:\n",
    "        \"\"\"Extract audio from video and process it with the audio embedding module.\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to the video file\n",
    "            timestamp: Timestamp for the file\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success or failure\n",
    "        \"\"\"\n",
    "        try:\n",
    "            file_name = os.path.basename(video_path)\n",
    "            logger.info(f\"Extracting audio from video: {file_name}\")\n",
    "            \n",
    "            # Create a temp file for the extracted audio\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:\n",
    "                temp_audio_path = temp_file.name\n",
    "            \n",
    "            # Check if video has audio streams before attempting extraction\n",
    "            has_audio = False\n",
    "            \n",
    "            # Define should_monitor flag - we want monitoring for larger videos\n",
    "            file_size_mb = os.path.getsize(video_path) / (1024 * 1024) if os.path.exists(video_path) else 0\n",
    "            should_monitor = file_size_mb > 50  # Monitor for videos larger than 50MB\n",
    "            \n",
    "            try:\n",
    "                # Check if the video actually has audio streams\n",
    "                # Use communicate() instead of capture_output to avoid forking issues\n",
    "                process = subprocess.Popen([\n",
    "                    'ffprobe', \n",
    "                    '-v', 'error', \n",
    "                    '-select_streams', 'a', \n",
    "                    '-show_entries', 'stream=codec_type', \n",
    "                    '-of', 'csv=p=0', \n",
    "                    video_path\n",
    "                ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                \n",
    "                stdout, stderr = process.communicate(timeout=30)\n",
    "                if 'audio' in stdout.decode():\n",
    "                    has_audio = True\n",
    "                else:\n",
    "                    logger.warning(f\"Video {file_name} does not contain any audio streams\")\n",
    "                    if os.path.exists(temp_audio_path):\n",
    "                        os.unlink(temp_audio_path)\n",
    "                    return False\n",
    "            except subprocess.TimeoutExpired:\n",
    "                logger.warning(f\"Timeout when checking audio streams in {file_name}\")\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.unlink(temp_audio_path)\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error checking audio streams in {file_name}: {str(e)}\")\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.unlink(temp_audio_path)\n",
    "                return False\n",
    "            \n",
    "            if not has_audio:\n",
    "                logger.info(f\"No audio streams found in video {file_name}, skipping audio extraction\")\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.unlink(temp_audio_path)\n",
    "                return False\n",
    "            \n",
    "            # Use ffmpeg to extract audio from video with timeout\n",
    "            try:\n",
    "                # Set a timeout for the extraction process (5 minutes)\n",
    "                extraction_timeout = 300\n",
    "                \n",
    "                logger.info(f\"Starting audio extraction from {file_name} with {extraction_timeout}s timeout\")\n",
    "                \n",
    "                # Use optimized ffmpeg settings to make extraction more likely to succeed:\n",
    "                # 1. Use acodec=pcm_s16le for simple, reliable audio codec\n",
    "                # 2. Limit to mono audio (1 channel) to reduce file size\n",
    "                # 3. Set lower sample rate (16kHz is enough for speech)\n",
    "                # 4. Add -vn to completely disable video processing\n",
    "                # 5. Add -stats_period to get more frequent progress updates\n",
    "                cmd = [\n",
    "                    'ffmpeg', \n",
    "                    '-i', video_path,       # Input file\n",
    "                    '-vn',                  # Disable video processing\n",
    "                    '-acodec', 'pcm_s16le', # Simple PCM audio codec\n",
    "                    '-ar', '16000',         # 16kHz sample rate (enough for speech)\n",
    "                    '-ac', '1',             # Mono audio\n",
    "                    '-stats_period', '5',   # Show stats every 5 seconds\n",
    "                    '-y',                   # Overwrite output \n",
    "                    temp_audio_path         # Output file\n",
    "                ]\n",
    "                \n",
    "                # Run ffmpeg synchronously with timeout\n",
    "                try:\n",
    "                    # Use a better way to monitor progress and handle timeouts\n",
    "                    process = subprocess.Popen(\n",
    "                        cmd, \n",
    "                        stdout=subprocess.PIPE, \n",
    "                        stderr=subprocess.PIPE\n",
    "                    )\n",
    "                    \n",
    "                    # Monitor process with periodic checks (also good for container health)\n",
    "                    start_time = time.time()\n",
    "                    last_check_time = start_time\n",
    "                    check_interval = 30  # Check Milvus every 30 seconds during extraction\n",
    "                    \n",
    "                    # Monitor the extraction process\n",
    "                    while process.poll() is None:\n",
    "                        # Check if we've exceeded timeout\n",
    "                        if time.time() - start_time > extraction_timeout:\n",
    "                            logger.warning(f\"Audio extraction timed out after {extraction_timeout}s, terminating process\")\n",
    "                            process.terminate()\n",
    "                            time.sleep(2)  # Give it a moment to terminate gracefully\n",
    "                            if process.poll() is None:\n",
    "                                process.kill()  # Force kill if it didn't terminate\n",
    "                            if os.path.exists(temp_audio_path):\n",
    "                                os.unlink(temp_audio_path)\n",
    "                            return False\n",
    "                        \n",
    "                        # Periodically check if Milvus is still healthy\n",
    "                        if should_monitor and (time.time() - last_check_time) > check_interval:\n",
    "                            logger.info(f\"Checking Milvus container during audio extraction at {int(time.time() - start_time)}s...\")\n",
    "                            self.ensure_milvus_running()\n",
    "                            last_check_time = time.time()\n",
    "                        \n",
    "                        time.sleep(1)  # Sleep to avoid busy waiting\n",
    "                    \n",
    "                    # Check exit code\n",
    "                    if process.returncode != 0:\n",
    "                        stderr = process.stderr.read().decode() if process.stderr else \"No error output\"\n",
    "                        logger.error(f\"ffmpeg failed with return code {process.returncode}: {stderr}\")\n",
    "                        if os.path.exists(temp_audio_path):\n",
    "                            os.unlink(temp_audio_path)\n",
    "                        return False\n",
    "                    \n",
    "                    logger.info(f\"Successfully extracted audio from video: {file_name}\")\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    logger.warning(f\"Audio extraction from {file_name} timed out after {extraction_timeout}s\")\n",
    "                    if os.path.exists(temp_audio_path):\n",
    "                        os.unlink(temp_audio_path)\n",
    "                    return False\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    logger.error(f\"ffmpeg failed with return code {e.returncode}\")\n",
    "                    logger.error(f\"ffmpeg stdout: {e.stdout.decode() if e.stdout else 'None'}\")\n",
    "                    logger.error(f\"ffmpeg stderr: {e.stderr.decode() if e.stderr else 'None'}\")\n",
    "                    if os.path.exists(temp_audio_path):\n",
    "                        os.unlink(temp_audio_path)\n",
    "                    return False\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during audio extraction from {file_name}: {str(e)}\")\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.unlink(temp_audio_path)\n",
    "                return False\n",
    "            \n",
    "            # Verify the audio file was created and has content\n",
    "            if not os.path.exists(temp_audio_path) or os.path.getsize(temp_audio_path) == 0:\n",
    "                logger.error(f\"Audio extraction produced no output for video {file_name}\")\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.unlink(temp_audio_path)\n",
    "                return False\n",
    "            \n",
    "            # Process the extracted audio with timeout\n",
    "            try:\n",
    "                logger.info(f\"Processing extracted audio from {file_name}\")\n",
    "                \n",
    "                # Create unique source identifier for the audio\n",
    "                audio_source = f\"{video_path}#audio\"\n",
    "                \n",
    "                # Reduced processing - just get transcription\n",
    "                transcription = self.transcribe_audio(temp_audio_path)\n",
    "                if not transcription or len(transcription.strip()) == 0:\n",
    "                    logger.warning(f\"No transcription generated for audio from {file_name}\")\n",
    "                    if os.path.exists(temp_audio_path):\n",
    "                        os.unlink(temp_audio_path)\n",
    "                    return False\n",
    "                \n",
    "                # Generate text embedding for transcription - use CLIP_TEXT for consistency\n",
    "                # This ensures dimension compatibility with Milvus\n",
    "                text_embedding = self.generate_embedding(transcription, EmbeddingType.CLIP_TEXT)\n",
    "                \n",
    "                # Add to Milvus\n",
    "                content = f\"Audio from video: {file_name}\\nTranscription: {transcription}\"\n",
    "                \n",
    "                # Ensure Milvus is running before attempting to store\n",
    "                self.ensure_milvus_running()\n",
    "                \n",
    "                self.add_to_milvus(content, text_embedding, \"audio\", audio_source, timestamp)\n",
    "                logger.info(f\"Successfully processed audio from video: {file_name}\")\n",
    "                \n",
    "                # Clean up\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.unlink(temp_audio_path)\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process extracted audio from video {file_name}: {str(e)}\")\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.unlink(temp_audio_path)\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in extract_and_process_audio_from_video for {video_path}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def process_github_repo(self, repo_name: str = None) -> int:\n",
    "        \"\"\"Process GitHub repository with timestamp checking.\"\"\"\n",
    "        # First check if GitHub processing is enabled in config\n",
    "        if not self.config.sources.get(\"github\", {}).enabled:\n",
    "            logger.info(\"GitHub source is disabled in configuration\")\n",
    "            return 0\n",
    "        \n",
    "        if not self.github_token:\n",
    "            logger.error(\"GitHub token is not provided\")\n",
    "            raise ValueError(\"GitHub token is not provided\")\n",
    "        \n",
    "        processed_count = 0\n",
    "        total_checked_count = 0\n",
    "        skipped_count = 0\n",
    "        new_files_count = 0\n",
    "        modified_files_count = 0\n",
    "        github_client = Github(self.github_token)\n",
    "        \n",
    "        try:\n",
    "            repos = [github_client.get_repo(repo_name)] if repo_name else github_client.get_user().get_repos()\n",
    "            \n",
    "            logger.info(f\"Found {len(list(repos))} GitHub repositories to check\")\n",
    "            \n",
    "            for repo in repos:\n",
    "                logger.info(f\"Processing GitHub repository: {repo.name}\")\n",
    "                contents = repo.get_contents(\"\")\n",
    "                \n",
    "                # Get all content first with timestamps\n",
    "                all_files = []\n",
    "                repo_files_count = 0\n",
    "                \n",
    "                while contents:\n",
    "                    file_content = contents.pop(0)\n",
    "                    \n",
    "                    if file_content.type == \"dir\":\n",
    "                        contents.extend(repo.get_contents(file_content.path))\n",
    "                    else:\n",
    "                        repo_files_count += 1\n",
    "                        file_extension = os.path.splitext(file_content.path)[1].lower()\n",
    "                        is_supported, doc_type = self.config.is_supported(file_extension, \"github\")\n",
    "                        \n",
    "                        if is_supported:\n",
    "                            total_checked_count += 1\n",
    "                            # Get last commit for this file - use correct parameter\n",
    "                            try:\n",
    "                                # First try with 'number' parameter (newer versions)\n",
    "                                commits = repo.get_commits(path=file_content.path, number=1)\n",
    "                            except Exception as e:\n",
    "                                if \"unexpected keyword argument 'number'\" in str(e):\n",
    "                                    # Fall back to older API style\n",
    "                                    try:\n",
    "                                        # Try with per_page parameter\n",
    "                                        commits = repo.get_commits(path=file_content.path, per_page=1)\n",
    "                                    except Exception as e2:\n",
    "                                        if \"unexpected keyword argument 'per_page'\" in str(e2):\n",
    "                                            # Last resort - no parameters\n",
    "                                            commits = repo.get_commits(path=file_content.path)\n",
    "                                        else:\n",
    "                                            raise e2\n",
    "                                else:\n",
    "                                    raise e\n",
    "                                \n",
    "                            # Get first commit from iterator\n",
    "                            last_commit = None\n",
    "                            try:\n",
    "                                last_commit = next(iter(commits), None)\n",
    "                            except Exception as e:\n",
    "                                logger.warning(f\"Error getting commit for {file_content.path}: {str(e)}\")\n",
    "                            \n",
    "                            if last_commit:\n",
    "                                timestamp = int(last_commit.commit.author.date.timestamp())\n",
    "                                # Ensure consistent source path format for GitHub files\n",
    "                                # Format: github://{owner}/{repo}/{path} without double slashes\n",
    "                                repo_full_name = repo.full_name\n",
    "                                file_path = file_content.path\n",
    "                                source_path = f\"github://{repo_full_name}/{file_path}\"\n",
    "                                # Make sure there are no double slashes in the path portion (after github://)\n",
    "                                protocol = \"github://\"\n",
    "                                if source_path.startswith(protocol):\n",
    "                                    path_part = source_path[len(protocol):]\n",
    "                                    path_part = re.sub('//+', '/', path_part)\n",
    "                                    source_path = f\"{protocol}{path_part}\"\n",
    "                                \n",
    "                                # Check if this file needs updating in Milvus\n",
    "                                needs_update = self._check_if_file_needs_update(source_path, timestamp)\n",
    "                                \n",
    "                                # File will be processed if:\n",
    "                                # 1. It's not in Milvus yet (new file)\n",
    "                                # 2. It's been modified since last ingestion\n",
    "                                if needs_update:\n",
    "                                    # Check if this is a new file or just modified\n",
    "                                    is_new_file = self._is_new_file(source_path)\n",
    "                                    if is_new_file:\n",
    "                                        new_files_count += 1\n",
    "                                    else:\n",
    "                                        modified_files_count += 1\n",
    "                                        \n",
    "                                    all_files.append({\n",
    "                                        \"content\": file_content,\n",
    "                                        \"timestamp\": timestamp,\n",
    "                                        \"doc_type\": doc_type,\n",
    "                                        \"source_path\": source_path,\n",
    "                                        \"is_new\": is_new_file\n",
    "                                    })\n",
    "                                else:\n",
    "                                    skipped_count += 1\n",
    "                \n",
    "                logger.info(f\"Checked {repo_files_count} files in repository {repo.name}\")\n",
    "                logger.info(f\"Found {len(all_files)} files to process: {new_files_count} new, {modified_files_count} modified\")\n",
    "                \n",
    "                # Now process only files that need updating\n",
    "                for file_data in all_files:\n",
    "                    try:\n",
    "                        content = file_data[\"content\"]\n",
    "                        raw_content = content.decoded_content.decode('utf-8')\n",
    "                        doc_config = self.config.sources[\"github\"].document_types[file_data[\"doc_type\"]]\n",
    "                        \n",
    "                        embedding = self.generate_embedding(raw_content, doc_config.embedding_type)\n",
    "                        \n",
    "                        # Add to Milvus with timestamp\n",
    "                        self.add_to_milvus_with_timestamp(\n",
    "                            raw_content,\n",
    "                            embedding,\n",
    "                            file_data[\"doc_type\"],\n",
    "                            file_data[\"source_path\"],\n",
    "                            file_data[\"timestamp\"]\n",
    "                        )\n",
    "                        processed_count += 1\n",
    "                        \n",
    "                        if file_data.get(\"is_new\", False):\n",
    "                            logger.info(f\"Added NEW file to Milvus: {file_data['source_path']}\")\n",
    "                        else:\n",
    "                            logger.info(f\"Updated MODIFIED file in Milvus: {file_data['source_path']}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing {file_data['source_path']}: {str(e)}\")\n",
    "            \n",
    "            logger.info(f\"GitHub summary: checked {total_checked_count} files, processed {processed_count} ({new_files_count} new, {modified_files_count} modified), skipped {skipped_count}\")\n",
    "            return processed_count\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error accessing GitHub: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def _check_if_file_needs_update(self, source_path, timestamp):\n",
    "        \"\"\"Check if file needs to be updated in Milvus based on timestamp comparison.\n",
    "        \n",
    "        Args:\n",
    "            source_path: The source identifier for the file in Milvus\n",
    "            timestamp: The current timestamp of the file\n",
    "            \n",
    "        Returns:\n",
    "            True if file needs to be updated (either not in Milvus or has a newer timestamp)\n",
    "        \"\"\"\n",
    "        # Add retry mechanism for Milvus operations\n",
    "        max_retries = 3  # Reduced from 5\n",
    "        retry_delay = 1  # seconds - Reduced from 2\n",
    "        query_timeout = 10  # seconds timeout for Milvus query\n",
    "        \n",
    "        # Static cache for file update checks (class variable)\n",
    "        if not hasattr(self.__class__, '_file_check_cache'):\n",
    "            self.__class__._file_check_cache = {}\n",
    "            \n",
    "        # Check cache first\n",
    "        cache_key = f\"{source_path}:{timestamp}\"\n",
    "        if cache_key in self.__class__._file_check_cache:\n",
    "            cached_result = self.__class__._file_check_cache[cache_key]\n",
    "            logger.debug(f\"Using cached result for {source_path}: {cached_result}\")\n",
    "            if not cached_result:\n",
    "                logger.info(f\"UNCHANGED FILE - skipping (cached): {source_path}\")\n",
    "            return cached_result\n",
    "            \n",
    "        # Start timing the operation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Use a more direct query approach with a timeout\n",
    "                # Escape special characters in source path to avoid SQL injection in query\n",
    "                escaped_source = source_path.replace(\"'\", \"''\").replace('\"', '\"\"')\n",
    "                search_params = {\"expr\": f'source == \"{escaped_source}\"'}\n",
    "            \n",
    "                # Set a timeout for the query operation\n",
    "                signal.alarm(query_timeout)\n",
    "                try:\n",
    "                    # Query Milvus for existing record - use optimized query with only needed fields\n",
    "                    results = self.collection.query(\n",
    "                        expr=search_params[\"expr\"],\n",
    "                        output_fields=[\"timestamp\"],\n",
    "                        limit=1  # Only need one result\n",
    "                    )\n",
    "                    # Reset the alarm\n",
    "                    signal.alarm(0)\n",
    "                except TimeoutError:\n",
    "                    logger.warning(f\"Milvus query timed out after {query_timeout}s for {source_path}\")\n",
    "                    # If we timeout, assume the file needs to be updated\n",
    "                    self.__class__._file_check_cache[cache_key] = True\n",
    "                    return True\n",
    "            \n",
    "                # If no results, file is not in database yet, needs processing\n",
    "                if not results:\n",
    "                    logger.info(f\"NEW FILE DETECTED - not in database, will process: {source_path}\")\n",
    "                    self.__class__._file_check_cache[cache_key] = True\n",
    "                    return True\n",
    "            \n",
    "                # Get existing timestamp and ensure it's an integer for comparison\n",
    "                try:\n",
    "                    existing_timestamp = int(results[0].get(\"timestamp\", 0))\n",
    "                    # Ensure current timestamp is also an integer\n",
    "                    current_timestamp = int(timestamp)\n",
    "                    needs_update = current_timestamp > existing_timestamp\n",
    "                except (TypeError, ValueError) as e:\n",
    "                    logger.warning(f\"Error converting timestamps for comparison: {str(e)}\")\n",
    "                    # If there's an issue with timestamp comparison, assume we need to update\n",
    "                    needs_update = True\n",
    "            \n",
    "                # Log the result and timing information\n",
    "                elapsed_time = time.time() - start_time\n",
    "                if needs_update:\n",
    "                        logger.info(f\"MODIFIED FILE - changed since last ingestion, will update: {source_path} (in {elapsed_time:.2f}s)\")\n",
    "                else:\n",
    "                        logger.info(f\"UNCHANGED FILE - skipping: {source_path} (in {elapsed_time:.2f}s)\")\n",
    "                \n",
    "                # Cache the result\n",
    "                self.__class__._file_check_cache[cache_key] = needs_update\n",
    "                return needs_update\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Reset the alarm to prevent lingering timeouts\n",
    "                signal.alarm(0)\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.warning(f\"Milvus query failed (attempt {attempt+1}/{max_retries}): {str(e)}\")\n",
    "                    logger.warning(f\"Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    # Increase delay for next attempt (exponential backoff)\n",
    "                    retry_delay *= 2\n",
    "                    \n",
    "                    # Try to reconnect to Milvus if the error appears to be connection-related\n",
    "                    if \"connect\" in str(e).lower() or \"connection\" in str(e).lower():\n",
    "                        try:\n",
    "                            logger.info(\"Attempting to reconnect to Milvus...\")\n",
    "                            connections.disconnect(\"default\")\n",
    "                            time.sleep(1)\n",
    "                            connections.connect(host='localhost', port='19530')\n",
    "                            logger.info(\"Reconnected to Milvus\")\n",
    "                        except Exception as reconnect_error:\n",
    "                            logger.warning(f\"Failed to reconnect to Milvus: {str(reconnect_error)}\")\n",
    "                else:\n",
    "                    logger.error(f\"ERROR checking if file needs update: {str(e)}\")\n",
    "                    logger.error(f\"For source path: {source_path}\")\n",
    "                    # If there's an error checking, assume we need to update to be safe\n",
    "                    self.__class__._file_check_cache[cache_key] = True\n",
    "            return True\n",
    "\n",
    "    def get_onenote_access_token(self) -> str:\n",
    "        \"\"\"Get access token for OneNote API using interactive authentication with token caching.\n",
    "        \n",
    "        Tokens are cached locally to avoid frequent authentication prompts.\n",
    "        - Access tokens expire after ~1 hour\n",
    "        - Refresh tokens expire after ~90 days of inactivity\n",
    "        \n",
    "        The method will attempt silent token refresh when possible and fall back\n",
    "        to interactive authentication when needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a token cache file\n",
    "            cache_file = os.path.join(os.path.expanduser(\"~\"), \".onenote_token_cache.json\")\n",
    "            \n",
    "            # Define token cache\n",
    "            token_cache = msal.SerializableTokenCache()\n",
    "            \n",
    "            # Load the token cache from file if it exists\n",
    "            if os.path.exists(cache_file):\n",
    "                with open(cache_file, 'r') as cache_file_handle:\n",
    "                    cache_data = cache_file_handle.read()\n",
    "                    if cache_data:\n",
    "                        token_cache.deserialize(cache_data)\n",
    "                        logger.info(\"Loaded authentication token from cache\")\n",
    "            \n",
    "            # Create MSAL app with the token cache\n",
    "            app = msal.PublicClientApplication(\n",
    "                self.microsoft_client_id,\n",
    "                authority=\"https://login.microsoftonline.com/consumers\",\n",
    "                token_cache=token_cache  # Set the token cache\n",
    "            )\n",
    "\n",
    "            # Define the scopes - use the correct format for Microsoft Graph API\n",
    "            scopes = [\"https://graph.microsoft.com/Notes.Read.All\",\n",
    "                     \"https://graph.microsoft.com/Notes.Read\",\n",
    "                     \"https://graph.microsoft.com/User.Read\"]\n",
    "            \n",
    "            # First, try to acquire token silently from cache\n",
    "            accounts = app.get_accounts()\n",
    "            if accounts:\n",
    "                logger.info(\"Found cached account, attempting silent token acquisition\")\n",
    "                result = app.acquire_token_silent(scopes, account=accounts[0])\n",
    "                if result and \"access_token\" in result:\n",
    "                    logger.info(\"Successfully acquired token silently from cache\")\n",
    "                    \n",
    "                    # Save cache for next time\n",
    "                    if token_cache.has_state_changed:\n",
    "                        with open(cache_file, 'w') as cache_file_handle:\n",
    "                            cache_file_handle.write(token_cache.serialize())\n",
    "                            logger.info(\"Token cache updated\")\n",
    "                    \n",
    "                    return result[\"access_token\"]\n",
    "                else:\n",
    "                    if result and \"error\" in result:\n",
    "                        # Check for specific error conditions\n",
    "                        if result.get(\"error\") == \"invalid_grant\":\n",
    "                            logger.warning(\"Refresh token has expired (occurs after ~90 days of inactivity)\")\n",
    "                            logger.warning(\"Interactive authentication will be required\")\n",
    "                        else:\n",
    "                            logger.warning(f\"Silent token acquisition failed: {result.get('error')}\")\n",
    "                            logger.warning(f\"Error description: {result.get('error_description')}\")\n",
    "                    else:\n",
    "                        logger.info(\"Silent token acquisition failed, cache may be expired\")\n",
    "            \n",
    "            # If silent acquisition fails, fall back to interactive login\n",
    "            logger.info(\"No valid cached token found, initiating interactive authentication...\")\n",
    "            logger.info(\"NOTE: You will need to re-authenticate if it has been >90 days since last use\")\n",
    "            \n",
    "            result = app.acquire_token_interactive(scopes=scopes)\n",
    "\n",
    "            if \"access_token\" in result:\n",
    "                logger.info(\"Successfully acquired access token through interactive login\")\n",
    "                \n",
    "                # Save cache for next time\n",
    "                if token_cache.has_state_changed:\n",
    "                    with open(cache_file, 'w') as cache_file_handle:\n",
    "                        cache_file_handle.write(token_cache.serialize())\n",
    "                        logger.info(\"Token cache saved\")\n",
    "                \n",
    "                return result[\"access_token\"]\n",
    "            else:\n",
    "                error_msg = result.get(\"error_description\", \"Unknown error\")\n",
    "                error_code = result.get(\"error\", \"unknown_error\")\n",
    "                \n",
    "                logger.error(f\"Interactive authentication failed: {error_code}\")\n",
    "                logger.error(f\"Error details: {error_msg}\")\n",
    "                \n",
    "                if \"AADSTS65001\" in error_msg:  # User consent required\n",
    "                    logger.error(\"You need to provide admin consent for this application\")\n",
    "                elif \"AADSTS50126\" in error_msg:  # Invalid credentials\n",
    "                    logger.error(\"Invalid username or password\")\n",
    "                elif \"AADSTS50128\" in error_msg or \"AADSTS50059\" in error_msg:  # Tenant issues\n",
    "                    logger.error(\"Tenant validation failed - make sure you're using the correct tenant ID\")\n",
    "                    \n",
    "                logger.info(\"\\nPlease verify in Azure Portal:\")\n",
    "                logger.info(\"1. Go to Azure Portal > App Registrations > Your App\")\n",
    "                logger.info(\"2. Click on 'Authentication' in the left menu\")\n",
    "                logger.info(\"3. Under 'Platform configurations', ensure 'Mobile and desktop applications' is added\")\n",
    "                logger.info(\"4. Check the box for 'https://login.microsoftonline.com/common/oauth2/nativeclient'\")\n",
    "                logger.info(\"5. Under 'Default client type', ensure 'Yes' is selected for 'Treat application as a public client'\")\n",
    "                logger.info(\"6. Under 'API permissions', ensure you have:\")\n",
    "                logger.info(\"   - User.Read\")\n",
    "                logger.info(\"   - Notes.Read.All\")\n",
    "                logger.info(\"   - Notes.Read\")\n",
    "                logger.info(\"\\nDebug information:\")\n",
    "                logger.info(f\"Client ID: {self.microsoft_client_id[:10]}... (truncated)\")\n",
    "                logger.info(f\"Authority: https://login.microsoftonline.com/consumers\")\n",
    "                raise ConfigurationError(f\"Authentication error: {error_code} - {error_msg}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting OneNote access token: {str(e)}\")\n",
    "            logger.error(\"Please verify your Azure AD app configuration and try again.\")\n",
    "            raise\n",
    "\n",
    "    def make_request_with_retry(self, url: str, headers: Dict[str, str], max_retries: int = 3) -> Optional[requests.Response]:\n",
    "        \"\"\"Make an HTTP request with retry logic for rate limiting.\"\"\"\n",
    "        # Log the request URL (truncate if too long)\n",
    "        display_url = url if len(url) < 100 else url[:97] + \"...\"\n",
    "        logger.info(f\"Making request to: {display_url}\")\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                \n",
    "                # Log response status and headers\n",
    "                logger.info(f\"Response status: {response.status_code}\")\n",
    "                \n",
    "                # Log relevant headers for debugging\n",
    "                important_headers = [\"x-ms-request-id\", \"retry-after\", \"x-ms-throttle-information\", \n",
    "                                    \"x-ms-throttle-categories\", \"x-ms-diagnostics\"]\n",
    "                log_headers = {k: v for k, v in response.headers.items() \n",
    "                              if k.lower() in [h.lower() for h in important_headers]}\n",
    "                \n",
    "                if log_headers:\n",
    "                    logger.info(f\"Response headers: {json.dumps(dict(log_headers))}\")\n",
    "                \n",
    "                # Handle rate limiting with retry\n",
    "                if response.status_code == 429:  # Rate limited\n",
    "                    retry_after = int(response.headers.get('Retry-After', 60))\n",
    "                    logger.warning(f\"Rate limited. Waiting {retry_after} seconds...\")\n",
    "                    # Log response body if it contains useful information\n",
    "                    try:\n",
    "                        error_content = response.json()\n",
    "                        logger.warning(f\"Rate limiting details: {json.dumps(error_content)}\")\n",
    "                    except:\n",
    "                        if response.text:\n",
    "                            logger.warning(f\"Rate limiting response text: {response.text[:200]}...\")\n",
    "                    time.sleep(retry_after)\n",
    "                    continue\n",
    "                \n",
    "                # For non-successful responses except 404 (not found)\n",
    "                if response.status_code >= 400 and response.status_code != 404:\n",
    "                    # Log error response body\n",
    "                    try:\n",
    "                        error_content = response.json()\n",
    "                        logger.warning(f\"Error response: {json.dumps(error_content)}\")\n",
    "                    except:\n",
    "                        if response.text:\n",
    "                            logger.warning(f\"Error response text: {response.text[:200]}...\")\n",
    "                            \n",
    "                    if attempt < max_retries - 1:\n",
    "                        # Retry with exponential backoff for server errors (5xx)\n",
    "                        if response.status_code >= 500:\n",
    "                            wait_time = 2 ** attempt\n",
    "                            logger.warning(f\"Server error ({response.status_code}). Retrying in {wait_time} seconds...\")\n",
    "                            time.sleep(wait_time)\n",
    "                            continue\n",
    "                        # For client errors (4xx), only retry certain ones\n",
    "                        elif response.status_code in [400, 401, 403]:\n",
    "                            if response.status_code == 401:  # Unauthorized - token might be expired\n",
    "                                logger.warning(\"Unauthorized request. Token might be expired.\")\n",
    "                            elif response.status_code == 400:\n",
    "                                logger.warning(\"Bad request. This could be due to malformed request or resource limitations.\")\n",
    "                            logger.warning(f\"Client error: {response.status_code} for URL: {url}\")\n",
    "                    # Don't raise for status, just return the response with error code\n",
    "                    return response\n",
    "                \n",
    "                # Success or 404, return as is\n",
    "                if response.status_code == 404:\n",
    "                    logger.info(\"Resource not found (404)\")\n",
    "                else:\n",
    "                    # Log success response summary for certain endpoints\n",
    "                    if \"resources\" in url:\n",
    "                        try:\n",
    "                            data = response.json()\n",
    "                            if \"value\" in data:\n",
    "                                logger.info(f\"Found {len(data['value'])} resources in response\")\n",
    "                                # Log the first few resources to help with debugging\n",
    "                                if data['value']:\n",
    "                                    sample = data['value'][:2]  # Just show first 2 for brevity\n",
    "                                    logger.info(f\"Resource sample: {json.dumps(sample)}\")\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Request failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    logger.warning(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.error(f\"Failed after {max_retries} attempts\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def process_onenote_content(self) -> int:\n",
    "        \"\"\"Process and ingest OneNote content into vector database.\"\"\"\n",
    "        if not self.config.sources.get(\"onenote\", {}).enabled:\n",
    "            logger.info(\"OneNote source is disabled in configuration\")\n",
    "            return 0\n",
    "\n",
    "        # Check if we have all required credentials\n",
    "        if not self.microsoft_client_id:\n",
    "            logger.warning(\"Microsoft Client ID not provided. OneNote processing will be skipped.\")\n",
    "            return 0\n",
    "\n",
    "        try:\n",
    "            # Get token with interactive authentication\n",
    "            logger.info(\"Acquiring Microsoft Graph API token for OneNote access...\")\n",
    "            try:\n",
    "                self.onenote_token = self.get_onenote_access_token()\n",
    "                logger.info(\"Authentication successful\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Authentication failed: {str(e)}\")\n",
    "                return 0\n",
    "            \n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.onenote_token}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "            graph_api = \"https://graph.microsoft.com/v1.0\"\n",
    "            processed_count = 0\n",
    "\n",
    "            # Get user info to verify the token works\n",
    "            logger.info(\"Retrieving user information...\")\n",
    "            user_response = self.make_request_with_retry(f\"{graph_api}/me\", headers)\n",
    "            \n",
    "            if not user_response or user_response.status_code != 200:\n",
    "                logger.error(f\"Failed to get user info. Status code: {user_response.status_code if user_response else 'No response'}\")\n",
    "                return 0\n",
    "                \n",
    "            user_info = user_response.json()\n",
    "            user_display_name = user_info.get('displayName', 'Unknown User')\n",
    "            logger.info(f\"Processing OneNote content for user: {user_display_name}\")\n",
    "\n",
    "            # Get notebooks\n",
    "            logger.info(\"Retrieving OneNote notebooks...\")\n",
    "            notebooks_response = self.make_request_with_retry(f\"{graph_api}/me/onenote/notebooks\", headers)\n",
    "            if not notebooks_response:\n",
    "                logger.error(\"Failed to retrieve notebooks after retries\")\n",
    "                return 0\n",
    "\n",
    "            notebooks = notebooks_response.json().get(\"value\", [])\n",
    "            logger.info(f\"Found {len(notebooks)} notebooks\")\n",
    "\n",
    "            doc_configs = self.config.sources[\"onenote\"].document_types\n",
    "\n",
    "            # Process notebooks hierarchically\n",
    "            for notebook in notebooks:\n",
    "                processed_count += self._process_onenote_notebook(\n",
    "                    notebook, \n",
    "                    headers, \n",
    "                    graph_api, \n",
    "                    user_display_name, \n",
    "                    doc_configs\n",
    "                )\n",
    "                \n",
    "            logger.info(f\"Total OneNote items processed: {processed_count}\")\n",
    "            return processed_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing OneNote content: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def _process_onenote_notebook(self, notebook, headers, graph_api, user_display_name, doc_configs):\n",
    "        \"\"\"Process a single OneNote notebook.\"\"\"\n",
    "        processed_count = 0\n",
    "        notebook_id = notebook[\"id\"]\n",
    "        notebook_name = notebook[\"displayName\"]\n",
    "        logger.info(f\"Processing notebook: {notebook_name}\")\n",
    "\n",
    "        # Get sections\n",
    "        sections_response = self.make_request_with_retry(\n",
    "            f\"{graph_api}/me/onenote/notebooks/{notebook_id}/sections\", \n",
    "            headers\n",
    "        )\n",
    "        if not sections_response:\n",
    "            return 0\n",
    "\n",
    "        sections = sections_response.json().get(\"value\", [])\n",
    "        logger.info(f\"Found {len(sections)} sections in notebook: {notebook_name}\")\n",
    "        \n",
    "        # Process each section\n",
    "        for section in sections:\n",
    "            section_id = section[\"id\"]\n",
    "            section_name = section[\"displayName\"]\n",
    "            logger.info(f\"Processing section: {section_name}\")\n",
    "\n",
    "            # Get pages\n",
    "            pages_response = self.make_request_with_retry(\n",
    "                f\"{graph_api}/me/onenote/sections/{section_id}/pages\", \n",
    "                headers\n",
    "            )\n",
    "            if not pages_response:\n",
    "                continue\n",
    "\n",
    "            pages = pages_response.json().get(\"value\", [])\n",
    "            logger.info(f\"Found {len(pages)} pages in section: {section_name}\")\n",
    "            \n",
    "            # Process each page\n",
    "            for page in pages:\n",
    "                page_id = page[\"id\"]\n",
    "                page_title = page[\"title\"]\n",
    "                logger.info(f\"Processing page: {page_title}\")\n",
    "                \n",
    "                # Construct the source identifier\n",
    "                source_path = f\"onenote://{user_display_name}/{notebook_name}/{section_name}/{page_title}\"\n",
    "                \n",
    "                # Process page content\n",
    "                if doc_configs.get(\"note\", {}).enabled:\n",
    "                    processed_count += self._process_onenote_page_content(\n",
    "                        page_id, \n",
    "                        page_title, \n",
    "                        notebook_name,\n",
    "                        source_path,\n",
    "                        headers,\n",
    "                        graph_api\n",
    "                    )\n",
    "\n",
    "                # Process page attachments\n",
    "                processed_count += self._process_onenote_attachments(\n",
    "                    page_id,\n",
    "                    page_title,\n",
    "                    notebook_name,\n",
    "                    source_path,\n",
    "                    headers,\n",
    "                    graph_api,\n",
    "                    doc_configs\n",
    "                )\n",
    "                \n",
    "        return processed_count\n",
    "        \n",
    "    def _process_onenote_page_content(self, page_id, page_title, notebook_name, source_path, headers, graph_api):\n",
    "        \"\"\"Process the content of a OneNote page.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Retrieving content for page: {page_title}\")\n",
    "            \n",
    "            # First get page metadata to check last modified time\n",
    "            page_metadata_response = self.make_request_with_retry(\n",
    "                f\"{graph_api}/me/onenote/pages/{page_id}\",\n",
    "                headers\n",
    "            )\n",
    "            \n",
    "            if not page_metadata_response or page_metadata_response.status_code != 200:\n",
    "                logger.warning(f\"Failed to get metadata for page {page_title}\")\n",
    "                return 0\n",
    "            \n",
    "            page_metadata = page_metadata_response.json()\n",
    "            \n",
    "            # Get the last modified time if available\n",
    "            last_modified_time = page_metadata.get('lastModifiedDateTime')\n",
    "            if last_modified_time:\n",
    "                # Convert ISO 8601 timestamp to epoch time\n",
    "                dt = dateutil.parser.parse(last_modified_time)\n",
    "                page_timestamp = int(dt.timestamp())\n",
    "            else:\n",
    "                # Use current time if modified time not available\n",
    "                page_timestamp = int(time.time())\n",
    "            \n",
    "            # Check if page needs updating\n",
    "            needs_update = self._check_if_file_needs_update(source_path, page_timestamp)\n",
    "            if not needs_update:\n",
    "                logger.info(f\"Skipping unchanged OneNote page: {page_title}\")\n",
    "                return 0\n",
    "            \n",
    "            # Get content if page needs updating\n",
    "            content_response = self.make_request_with_retry(\n",
    "                f\"{graph_api}/me/onenote/pages/{page_id}/content\",\n",
    "                headers\n",
    "            )\n",
    "            if not content_response or content_response.status_code != 200:\n",
    "                logger.warning(f\"Failed to get content for page {page_title}\")\n",
    "                return 0\n",
    "            \n",
    "            html_content = content_response.text\n",
    "            text_content = ' '.join(html_content.split())  # Simple HTML to text conversion\n",
    "            \n",
    "            # Generate CLIP text embedding\n",
    "            embedding = self.generate_embedding(text_content, EmbeddingType.CLIP_TEXT)\n",
    "            \n",
    "            # Store in Milvus with timestamp\n",
    "            self.add_to_milvus(text_content, embedding, \"note\", source_path, page_timestamp)\n",
    "            logger.info(f\"Added page content to vector database: {page_title}\")\n",
    "            return 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing page content: {str(e)}\")\n",
    "            return 0\n",
    "            \n",
    "    def _process_onenote_attachments(self, page_id, page_title, notebook_name, source_path, \n",
    "                                    headers, graph_api, doc_configs):\n",
    "        \"\"\"Process attachments in a OneNote page.\"\"\"\n",
    "        processed_count = 0\n",
    "        try:\n",
    "            # First get page metadata to check last modified time\n",
    "            page_metadata_response = self.make_request_with_retry(\n",
    "                f\"{graph_api}/me/onenote/pages/{page_id}\",\n",
    "                headers\n",
    "            )\n",
    "            \n",
    "            if not page_metadata_response or page_metadata_response.status_code != 200:\n",
    "                logger.warning(f\"Failed to get metadata for page {page_title}\")\n",
    "                return 0\n",
    "            \n",
    "            page_metadata = page_metadata_response.json()\n",
    "            \n",
    "            # Get the last modified time if available\n",
    "            last_modified_time = page_metadata.get('lastModifiedDateTime')\n",
    "            if last_modified_time:\n",
    "                # Convert ISO 8601 timestamp to epoch time\n",
    "                dt = dateutil.parser.parse(last_modified_time)\n",
    "                page_timestamp = int(dt.timestamp())\n",
    "            else:\n",
    "                # Use current time if modified time not available\n",
    "                page_timestamp = int(time.time())\n",
    "            \n",
    "            # For attachments, we apply a slightly different strategy than pages\n",
    "            # We don't skip the entire page if it's unchanged - instead we'll check each attachment\n",
    "            # This is because a page could be unchanged, but we want to process new attachments\n",
    "            \n",
    "            logger.info(f\"Retrieving resources for page: {page_title}\")\n",
    "            # Get page content with resources included - this is the correct way according to Microsoft docs\n",
    "            content_response = self.make_request_with_retry(\n",
    "                f\"{graph_api}/me/onenote/pages/{page_id}/content?includeResourcesContent=true\",\n",
    "                headers\n",
    "            )\n",
    "            \n",
    "            if not content_response or content_response.status_code != 200:\n",
    "                logger.warning(f\"Failed to retrieve content with resources for page {page_title}\")\n",
    "                return 0\n",
    "            \n",
    "            # Parse the HTML content to find resources\n",
    "            html_content = content_response.text\n",
    "            \n",
    "            # Import BeautifulSoup if not already imported\n",
    "            try:\n",
    "                from bs4 import BeautifulSoup\n",
    "            except ImportError:\n",
    "                logger.error(\"BeautifulSoup library is required but not installed. Install with: pip install beautifulsoup4\")\n",
    "                return 0\n",
    "            \n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # 1. Process images - according to docs, images are in <img> tags with src and data-fullres-src attributes\n",
    "            images = soup.find_all('img')\n",
    "            logger.info(f\"Found {len(images)} images in page: {page_title}\")\n",
    "            \n",
    "            for img in images:\n",
    "                try:\n",
    "                    # Use the full resolution image URL if available, or fall back to optimized version\n",
    "                    img_url = img.get('data-fullres-src') or img.get('src')\n",
    "                    if not img_url:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get image type from attributes\n",
    "                    img_type = img.get('data-fullres-src-type') or img.get('data-src-type') or 'image/jpeg'\n",
    "                    \n",
    "                    # Skip very small images - likely icons or UI elements\n",
    "                    # Fix: Handle floating point values in width/height\n",
    "                    try:\n",
    "                        width = img.get('width', '0')\n",
    "                        height = img.get('height', '0')\n",
    "                        \n",
    "                        # Convert to float first, then to int if needed\n",
    "                        width_float = float(width) if width and width.strip() else 0\n",
    "                        height_float = float(height) if height and height.strip() else 0\n",
    "                        \n",
    "                        # Skip small images\n",
    "                        if width_float < 50 or height_float < 50:\n",
    "                            continue\n",
    "                    except ValueError as e:\n",
    "                        # If conversion fails, log but don't crash\n",
    "                        logger.warning(f\"Error converting image dimensions for {img_url}: {str(e)}\")\n",
    "                        # Process the image anyway since we can't reliably determine its size\n",
    "                        \n",
    "                    # Create a meaningful name for the image\n",
    "                    img_name = img.get('alt') or f\"image_{hash(img_url)}\"\n",
    "                    attachment_source = f\"{source_path}/image/{img_name}\"\n",
    "                    \n",
    "                    # For attachments, we use the page's timestamp - attachments don't have their own timestamp\n",
    "                    # but they're tied to the page's modification time\n",
    "                    \n",
    "                    # Check if attachment needs updating based on page timestamp\n",
    "                    # This is because OneNote doesn't provide timestamps for individual attachments\n",
    "                    needs_update = self._check_if_file_needs_update(attachment_source, page_timestamp)\n",
    "                    if not needs_update:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process the image if this type is enabled in configuration\n",
    "                    if self._is_image_attachment(os.path.splitext(img_name)[1], img_type, doc_configs):\n",
    "                        processed_count += self._process_onenote_image(\n",
    "                            img_url, img_name, attachment_source, headers, page_timestamp)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing image in page {page_title}: {str(e)}\")\n",
    "            \n",
    "            # 2. Process file attachments - according to docs, objects are in <object> tags with data attribute\n",
    "            objects = soup.find_all('object')\n",
    "            logger.info(f\"Found {len(objects)} file attachments in page: {page_title}\")\n",
    "            \n",
    "            for obj in objects:\n",
    "                try:\n",
    "                    data_url = obj.get('data')\n",
    "                    if not data_url:\n",
    "                        continue\n",
    "                        \n",
    "                    # Get file info from object attributes\n",
    "                    file_type = obj.get('type', '')\n",
    "                    file_name = obj.get('data-attachment', f\"file_{hash(data_url)}\")\n",
    "                    file_extension = os.path.splitext(file_name)[1].lower() or '.bin'\n",
    "                    \n",
    "                    attachment_source = f\"{source_path}/attachment/{file_name}\"\n",
    "                    \n",
    "                    # Check if attachment needs updating based on page timestamp\n",
    "                    needs_update = self._check_if_file_needs_update(attachment_source, page_timestamp)\n",
    "                    if not needs_update:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process based on attachment type\n",
    "                    if self._is_document_attachment(file_extension, doc_configs):\n",
    "                        processed_count += self._process_onenote_document(\n",
    "                            data_url, file_name, attachment_source, headers, page_timestamp)\n",
    "                            \n",
    "                    elif self._is_audio_attachment(file_extension, file_type, doc_configs):\n",
    "                        processed_count += self._process_onenote_audio(\n",
    "                            data_url, file_name, attachment_source, headers, page_timestamp)\n",
    "                            \n",
    "                    elif self._is_video_attachment(file_extension, file_type, doc_configs):\n",
    "                        processed_count += self._process_onenote_video(\n",
    "                            data_url, file_name, attachment_source, headers, page_timestamp)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing file attachment in page {page_title}: {str(e)}\")\n",
    "                    \n",
    "            # 3. Process links that might contain attachments (some OneNote versions use <a> tags)\n",
    "            links = soup.find_all('a', {'data-attachment': True})\n",
    "            logger.info(f\"Found {len(links)} linked attachments in page: {page_title}\")\n",
    "            \n",
    "            for link in links:\n",
    "                try:\n",
    "                    href = link.get('href')\n",
    "                    if not href:\n",
    "                        continue\n",
    "                        \n",
    "                    file_name = link.get('data-attachment', link.text or f\"file_{hash(href)}\")\n",
    "                    file_extension = os.path.splitext(file_name)[1].lower() or '.bin'\n",
    "                    \n",
    "                    attachment_source = f\"{source_path}/attachment/{file_name}\"\n",
    "                    \n",
    "                    # Check if attachment needs updating based on page timestamp\n",
    "                    needs_update = self._check_if_file_needs_update(attachment_source, page_timestamp)\n",
    "                    if not needs_update:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process based on attachment type\n",
    "                    if self._is_document_attachment(file_extension, doc_configs):\n",
    "                        processed_count += self._process_onenote_document(\n",
    "                            href, file_name, attachment_source, headers, page_timestamp)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing linked attachment in page {page_title}: {str(e)}\")\n",
    "                    \n",
    "            return processed_count\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving attachments: {str(e)}\")\n",
    "            return 0\n",
    "            \n",
    "    # Helper methods for attachment type checking\n",
    "    \n",
    "    def _is_image_attachment(self, file_extension, content_type, doc_configs):\n",
    "        \"\"\"Check if attachment is an image.\"\"\"\n",
    "        return (doc_configs.get(\"image_attachment\", {}).enabled and \n",
    "                (file_extension in doc_configs[\"image_attachment\"].extensions or\n",
    "                 \"image\" in content_type))\n",
    "                 \n",
    "    def _is_document_attachment(self, file_extension, doc_configs):\n",
    "        \"\"\"Check if attachment is a document.\"\"\"\n",
    "        return (doc_configs.get(\"document_attachment\", {}).enabled and \n",
    "                file_extension in doc_configs[\"document_attachment\"].extensions)\n",
    "                \n",
    "    def _is_audio_attachment(self, file_extension, content_type, doc_configs):\n",
    "        \"\"\"Check if attachment is audio.\"\"\"\n",
    "        return (doc_configs.get(\"audio_attachment\", {}).enabled and \n",
    "                (file_extension in doc_configs[\"audio_attachment\"].extensions or\n",
    "                 \"audio\" in content_type))\n",
    "                 \n",
    "    def _is_video_attachment(self, file_extension, content_type, doc_configs):\n",
    "        \"\"\"Check if attachment is video.\"\"\"\n",
    "        return (doc_configs.get(\"video_attachment\", {}).enabled and \n",
    "                (file_extension in doc_configs[\"video_attachment\"].extensions or\n",
    "                 \"video\" in content_type))\n",
    "    \n",
    "    # OneNote attachment processing methods\n",
    "    \n",
    "    def _process_onenote_image(self, content_url, file_name, source, headers, page_timestamp):\n",
    "        \"\"\"Process an image attachment from OneNote.\"\"\"\n",
    "        logger.info(f\"Processing image attachment: {file_name}\")\n",
    "        \n",
    "        # Truncate image file_name if too long (for logging)\n",
    "        display_name = file_name\n",
    "        if len(display_name) > 100:\n",
    "            display_name = display_name[:97] + \"...\"\n",
    "        \n",
    "        logger.info(f\"Processing image attachment: {display_name}\")\n",
    "        \n",
    "        # If the URL is a direct resource URL from OneNote HTML, use it as is\n",
    "        image_response = self.make_request_with_retry(content_url, headers)\n",
    "        \n",
    "        if not image_response or image_response.status_code != 200:\n",
    "            logger.warning(f\"Failed to fetch image: {display_name}, status code: {image_response.status_code if image_response else 'no response'}\")\n",
    "            return 0\n",
    "            \n",
    "        try:\n",
    "            image_data = Image.open(io.BytesIO(image_response.content))\n",
    "            embedding = self.generate_embedding(image_data, EmbeddingType.CLIP_IMAGE)\n",
    "            \n",
    "            # Truncate source path if too long for Milvus (which has a limit of 256 chars)\n",
    "            truncated_source = source\n",
    "            if len(truncated_source) > 250:  # Leave some margin\n",
    "                # Keep the beginning and end for identification\n",
    "                prefix_length = 120\n",
    "                suffix_length = 120\n",
    "                truncated_source = f\"{truncated_source[:prefix_length]}...{truncated_source[-suffix_length:]}\"\n",
    "            \n",
    "            # Also truncate file_name for the content field\n",
    "            content_text = f\"Image: {file_name}\"\n",
    "            if len(content_text) > 250:\n",
    "                content_text = f\"Image: {file_name[:240]}...\"\n",
    "            \n",
    "            self.add_to_milvus(content_text, embedding, \"image_attachment\", truncated_source, page_timestamp)\n",
    "            logger.info(f\"Successfully processed image: {display_name}\")\n",
    "            return 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process image {display_name}: {str(e)}\")\n",
    "            return 0\n",
    "            \n",
    "    def _process_onenote_document(self, content_url, file_name, source, headers, page_timestamp):\n",
    "        \"\"\"Process a document attachment from OneNote.\"\"\"\n",
    "        logger.info(f\"Processing document attachment: {file_name}\")\n",
    "        \n",
    "        # If the URL is a direct resource URL from OneNote HTML, use it as is\n",
    "        doc_response = self.make_request_with_retry(content_url, headers)\n",
    "        \n",
    "        if not doc_response or doc_response.status_code != 200:\n",
    "            logger.warning(f\"Failed to fetch document: {file_name}, status code: {doc_response.status_code if doc_response else 'no response'}\")\n",
    "            return 0\n",
    "            \n",
    "        try:\n",
    "            # Save document temporarily\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as temp_file:\n",
    "                temp_file.write(doc_response.content)\n",
    "                temp_path = temp_file.name\n",
    "                \n",
    "            # Process with Tika\n",
    "            parsed_doc = self.process_document(temp_path)\n",
    "            \n",
    "            if parsed_doc.get('extraction_method') != 'failed':\n",
    "                embedding = self.generate_embedding(parsed_doc['content'], EmbeddingType.CLIP_TEXT)\n",
    "                self.add_to_milvus(parsed_doc['content'], embedding, \"document_attachment\", source, page_timestamp)\n",
    "                logger.info(f\"Successfully processed document: {file_name}\")\n",
    "                \n",
    "                # Clean up\n",
    "                os.unlink(temp_path)\n",
    "                return 1\n",
    "            \n",
    "            logger.warning(f\"Failed to extract content from document: {file_name}\")\n",
    "            os.unlink(temp_path)\n",
    "            return 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process document {file_name}: {str(e)}\")\n",
    "            return 0\n",
    "            \n",
    "    def _process_onenote_audio(self, content_url, file_name, source, headers, page_timestamp):\n",
    "        \"\"\"Process an audio attachment from OneNote.\"\"\"\n",
    "        logger.info(f\"Processing audio attachment: {file_name}\")\n",
    "        \n",
    "        # If the URL is a direct resource URL from OneNote HTML, use it as is\n",
    "        audio_response = self.make_request_with_retry(content_url, headers)\n",
    "        \n",
    "        if not audio_response or audio_response.status_code != 200:\n",
    "            logger.warning(f\"Failed to fetch audio: {file_name}, status code: {audio_response.status_code if audio_response else 'no response'}\")\n",
    "            return 0\n",
    "            \n",
    "        try:\n",
    "            # Save audio temporarily\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as temp_file:\n",
    "                temp_file.write(audio_response.content)\n",
    "                temp_path = temp_file.name\n",
    "                \n",
    "            # Transcribe audio\n",
    "            transcription = self.transcribe_audio(temp_path)\n",
    "            \n",
    "            # Generate text embedding for transcription\n",
    "            text_embedding = self.generate_embedding(transcription, EmbeddingType.CLIP_TEXT)\n",
    "            self.add_to_milvus(transcription, text_embedding, \"audio_transcription\", source, page_timestamp)\n",
    "            \n",
    "            # Generate audio embedding\n",
    "            # IMPORTANT: Use CLIP_TEXT for audio content as well to ensure dimension compatibility\n",
    "            # The previous code tried to use WAV2VEC which causes dimension mismatch with Milvus\n",
    "            audio_content = f\"Audio: {file_name}\"\n",
    "            audio_embedding = self.generate_embedding(audio_content, EmbeddingType.CLIP_TEXT)\n",
    "            self.add_to_milvus(audio_content, audio_embedding, \"audio_attachment\", source, page_timestamp)\n",
    "            \n",
    "            # Clean up\n",
    "            os.unlink(temp_path)\n",
    "            logger.info(f\"Successfully processed audio: {file_name}\")\n",
    "            return 2  # Added 2 items (transcription and audio)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process audio {file_name}: {str(e)}\")\n",
    "            return 0\n",
    "            \n",
    "    def _process_onenote_video(self, content_url, file_name, source, headers, page_timestamp):\n",
    "        \"\"\"Process a video attachment from OneNote.\"\"\"\n",
    "        logger.info(f\"Processing video attachment: {file_name}\")\n",
    "        \n",
    "        # If the URL is a direct resource URL from OneNote HTML, use it as is\n",
    "        video_response = self.make_request_with_retry(content_url, headers)\n",
    "        \n",
    "        if not video_response or video_response.status_code != 200:\n",
    "            logger.warning(f\"Failed to fetch video: {file_name}, status code: {video_response.status_code if video_response else 'no response'}\")\n",
    "            return 0\n",
    "            \n",
    "        try:\n",
    "            # Save video temporarily\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as temp_file:\n",
    "                temp_file.write(video_response.content)\n",
    "                temp_path = temp_file.name\n",
    "                \n",
    "            # Extract frames\n",
    "            frames = self.extract_video_frames(temp_path)\n",
    "            \n",
    "            processed_count = 0\n",
    "            for i, frame in enumerate(frames):\n",
    "                # Generate embedding for each frame\n",
    "                frame_embedding = self.generate_embedding(frame, EmbeddingType.CLIP_IMAGE)\n",
    "                frame_content = f\"Frame {i} from video {file_name}\"\n",
    "                self.add_to_milvus(frame_content, frame_embedding, \"video_attachment\", f\"{source}:frame_{i}\", page_timestamp)\n",
    "                processed_count += 1\n",
    "                \n",
    "            # Clean up\n",
    "            os.unlink(temp_path)\n",
    "            logger.info(f\"Successfully processed video: {file_name}, extracted {processed_count} frames\")\n",
    "            return processed_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process video {file_name}: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def _is_new_file(self, source_path):\n",
    "        \"\"\"Check if a file is new (not yet in Milvus).\n",
    "        \n",
    "        This is an optimized version with caching to prevent redundant queries.\n",
    "        \n",
    "        Args:\n",
    "            source_path: Path to the file\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if the file is new\n",
    "        \"\"\"\n",
    "        # Use class-level cache to avoid repeated queries\n",
    "        if not hasattr(self.__class__, '_new_file_cache'):\n",
    "            self.__class__._new_file_cache = {}\n",
    "            \n",
    "        # Return cached result if available\n",
    "        if source_path in self.__class__._new_file_cache:\n",
    "            return self.__class__._new_file_cache[source_path]\n",
    "            \n",
    "        try:\n",
    "            # Set a timeout for the query operation (5 seconds)\n",
    "            query_timeout = 5\n",
    "            \n",
    "            # Escape special characters\n",
    "            escaped_source = source_path.replace(\"'\", \"''\").replace('\"', '\"\"')\n",
    "            \n",
    "            # Set the alarm\n",
    "            signal.alarm(query_timeout)\n",
    "            try:\n",
    "                # Optimized query - only get existence info with minimal fields\n",
    "                results = self.collection.query(\n",
    "                    expr=f'source == \"{escaped_source}\"',\n",
    "                    output_fields=[\"id\"],\n",
    "                    limit=1\n",
    "                )\n",
    "                # Turn off the alarm\n",
    "                signal.alarm(0)\n",
    "                \n",
    "                is_new = len(results) == 0\n",
    "                # Cache the result\n",
    "                self.__class__._new_file_cache[source_path] = is_new\n",
    "                return is_new\n",
    "                \n",
    "            except TimeoutError:\n",
    "                logger.warning(f\"Query timed out when checking if {source_path} is new\")\n",
    "                # Turn off the alarm\n",
    "                signal.alarm(0)\n",
    "                # Assume it's not new to be safe\n",
    "                self.__class__._new_file_cache[source_path] = False\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Turn off the alarm\n",
    "            signal.alarm(0)\n",
    "            logger.warning(f\"Error checking if {source_path} is new: {str(e)}\")\n",
    "            # Assume it's not new to be safe\n",
    "            self.__class__._new_file_cache[source_path] = False\n",
    "            return False\n",
    "\n",
    "    def check_milvus_containers(self):\n",
    "        \"\"\"Check if Milvus containers are running and healthy.\"\"\"\n",
    "        try:\n",
    "            # Check if containers exist first\n",
    "            containers_exist = subprocess.run(\n",
    "                [\"docker\", \"ps\", \"-a\", \"--format\", \"{{.Names}}\", \"--filter\", \"name=milvus\"],\n",
    "                capture_output=True, text=True\n",
    "            ).stdout.strip().split('\\n')\n",
    "            \n",
    "            # Initialize status variables\n",
    "            standalone_status = \"not_found\"\n",
    "            standalone_health = \"unknown\"\n",
    "            etcd_status = \"not_found\"\n",
    "            etcd_health = \"unknown\"\n",
    "            minio_status = \"not_found\" \n",
    "            minio_health = \"unknown\"\n",
    "            \n",
    "            # Get container status with more reliable approach\n",
    "            if \"milvus-standalone\" in containers_exist:\n",
    "                standalone_info = subprocess.run(\n",
    "                    [\"docker\", \"inspect\", \"milvus-standalone\", \"--format\", \"{{.State.Status}}\"],\n",
    "                    capture_output=True, text=True\n",
    "                ).stdout.strip()\n",
    "                standalone_status = standalone_info if standalone_info else \"not_running\"\n",
    "                \n",
    "                # Get health status separately to avoid parsing issues\n",
    "                health_check = subprocess.run(\n",
    "                    [\"docker\", \"inspect\", \"milvus-standalone\", \"--format\", \"{{if .State.Health}}{{.State.Health.Status}}{{else}}none{{end}}\"],\n",
    "                    capture_output=True, text=True\n",
    "                ).stdout.strip()\n",
    "                standalone_health = health_check if health_check != \"none\" else \"unknown\"\n",
    "            \n",
    "            if \"milvus-etcd\" in containers_exist:\n",
    "                etcd_info = subprocess.run(\n",
    "                    [\"docker\", \"inspect\", \"milvus-etcd\", \"--format\", \"{{.State.Status}}\"],\n",
    "                    capture_output=True, text=True\n",
    "                ).stdout.strip()\n",
    "                etcd_status = etcd_info if etcd_info else \"not_running\"\n",
    "                \n",
    "                # For etcd, we assume it's healthy if running since it has no health check\n",
    "                etcd_health = \"assumed_healthy\" if etcd_status == \"running\" else \"unknown\"\n",
    "            \n",
    "            if \"milvus-minio\" in containers_exist:\n",
    "                minio_info = subprocess.run(\n",
    "                    [\"docker\", \"inspect\", \"milvus-minio\", \"--format\", \"{{.State.Status}}\"],\n",
    "                    capture_output=True, text=True\n",
    "                ).stdout.strip()\n",
    "                minio_status = minio_info if minio_info else \"not_running\"\n",
    "                \n",
    "                # Get health status separately\n",
    "                health_check = subprocess.run(\n",
    "                    [\"docker\", \"inspect\", \"milvus-minio\", \"--format\", \"{{if .State.Health}}{{.State.Health.Status}}{{else}}none{{end}}\"],\n",
    "                    capture_output=True, text=True\n",
    "                ).stdout.strip()\n",
    "                minio_health = health_check if health_check != \"none\" else \"unknown\"\n",
    "            \n",
    "            # Check if all containers are running\n",
    "            all_running = (\n",
    "                standalone_status == \"running\" and\n",
    "                etcd_status == \"running\" and\n",
    "                minio_status == \"running\"\n",
    "            )\n",
    "            \n",
    "            # Check if all containers are healthy\n",
    "            all_healthy = (\n",
    "                (standalone_health == \"healthy\" or standalone_health == \"starting\") and\n",
    "                (etcd_status == \"running\") and  # For etcd, we just check it's running\n",
    "                (minio_health == \"healthy\" or minio_health == \"starting\")\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Container status - Standalone: {standalone_status}/{standalone_health}, \" + \n",
    "                       f\"Etcd: {etcd_status}/{etcd_health}, \" + \n",
    "                       f\"Minio: {minio_status}/{minio_health}\")\n",
    "            \n",
    "            return all_running, all_healthy\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking container status: {str(e)}\")\n",
    "            return False, False\n",
    "    \n",
    "    def restart_milvus_containers(self):\n",
    "        \"\"\"Restart Milvus containers if they are down.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Attempting to restart Milvus containers...\")\n",
    "            \n",
    "            # Get the docker-compose file path\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            compose_file = os.path.join(current_dir, 'docker-compose.yml')\n",
    "            \n",
    "            # Check if docker-compose.yml exists\n",
    "            if not os.path.exists(compose_file):\n",
    "                logger.warning(f\"docker-compose.yml not found at {compose_file}\")\n",
    "                compose_file = 'docker-compose.yml'  # Try in current working directory\n",
    "            \n",
    "            # Stop any existing containers first\n",
    "            logger.info(\"Stopping any existing Milvus containers...\")\n",
    "            subprocess.run(['docker-compose', '-f', compose_file, 'down'], check=False)\n",
    "            \n",
    "            # Wait a moment for containers to fully stop\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Start services with docker-compose\n",
    "            logger.info(\"Starting Milvus and supporting services...\")\n",
    "            subprocess.run(['docker-compose', '-f', compose_file, 'up', '-d'], check=True)\n",
    "            logger.info(\"Milvus containers restarted\")\n",
    "            \n",
    "            # Wait for Milvus to be ready\n",
    "            max_retries = 30\n",
    "            retry_interval = 3\n",
    "            \n",
    "            # Test both potential health endpoints\n",
    "            endpoints = [\n",
    "                ('http://localhost:19530/v1/health', 'API endpoint'),\n",
    "                ('http://localhost:9091/healthz', 'Metrics endpoint')\n",
    "            ]\n",
    "            \n",
    "            for i in range(max_retries):\n",
    "                for endpoint, description in endpoints:\n",
    "                    try:\n",
    "                        # Try to connect to health endpoints\n",
    "                        response = requests.get(endpoint, timeout=5)\n",
    "                        if response.status_code < 400:  # Consider any non-error response as successful\n",
    "                            logger.info(f\"Milvus is ready at {description} (took {i * retry_interval} seconds)\")\n",
    "                            return True\n",
    "                    except Exception:\n",
    "                        pass  # Ignore errors and try the next endpoint\n",
    "                \n",
    "                # Also try connecting through pymilvus\n",
    "                try:\n",
    "                    connections.connect(host='localhost', port='19530', timeout=5)\n",
    "                    logger.info(f\"Milvus is ready via pymilvus connection (took {i * retry_interval} seconds)\")\n",
    "                    connections.disconnect(\"default\")\n",
    "                    return True\n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                # If we haven't succeeded yet, wait and retry\n",
    "                if i < max_retries - 1:\n",
    "                    time.sleep(retry_interval)\n",
    "                else:\n",
    "                    logger.error(\"Timed out waiting for Milvus to be ready after restart\")\n",
    "                    return False\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error restarting Milvus containers: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def ensure_milvus_running(self):\n",
    "        \"\"\"Ensure Milvus is running and healthy, restart if needed.\"\"\"\n",
    "        all_running, all_healthy = self.check_milvus_containers()\n",
    "        \n",
    "        if not all_running:\n",
    "            logger.warning(\"Milvus containers are not all running, attempting restart...\")\n",
    "            return self.restart_milvus_containers()\n",
    "        \n",
    "        if not all_healthy:\n",
    "            logger.warning(\"Milvus containers are running but not all healthy, attempting restart...\")\n",
    "            return self.restart_milvus_containers()\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _ensure_milvus_connection(self):\n",
    "        \"\"\"Ensure Milvus connection is active.\n",
    "        Simple version to fix the error without changing too much code.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try to check connection by getting server version\n",
    "            if not connections.has_connection(\"default\"):\n",
    "                logger.info(\"No active Milvus connection, connecting...\")\n",
    "                connections.connect(host='localhost', port='19530')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Milvus connection issue: {str(e)}\")\n",
    "            try:\n",
    "                # Try to reconnect\n",
    "                if connections.has_connection(\"default\"):\n",
    "                    connections.disconnect(\"default\")\n",
    "                time.sleep(1)\n",
    "                connections.connect(host='localhost', port='19530')\n",
    "                logger.info(\"Reconnected to Milvus\")\n",
    "                return True\n",
    "            except Exception as reconnect_e:\n",
    "                logger.error(f\"Failed to reconnect to Milvus: {str(reconnect_e)}\")\n",
    "                return False\n",
    "\n",
    "    def process_video(self, file_path, should_extract_text=True):\n",
    "        \"\"\"\n",
    "        Process video files extracting frames and audio if available\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Processing video file: {file_path}\")\n",
    "            \n",
    "            # Extract video frames\n",
    "            frames = self.extract_video_frames(file_path)\n",
    "            \n",
    "            if frames and len(frames) > 0:\n",
    "                logger.info(f\"Successfully extracted {len(frames)} frames\")\n",
    "                \n",
    "                # Process frames with CLIP Vision model\n",
    "                for i, frame in enumerate(frames):\n",
    "                    try:\n",
    "                        # Generate embedding for this frame\n",
    "                        embedding = self._generate_image_embedding(frame)\n",
    "                        \n",
    "                        if embedding is not None:\n",
    "                            # Create a content identifier for this frame\n",
    "                            content = f\"Video frame {i+1} from {os.path.basename(file_path)}\"\n",
    "                            \n",
    "                            # Add to Milvus\n",
    "                            self.add_to_milvus(content, embedding, \"video_frame\", file_path, \n",
    "                                              int(time.time()), chunk_index=i)\n",
    "                            \n",
    "                            logger.debug(f\"Added frame {i+1} to Milvus\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing frame {i+1}: {str(e)}\")\n",
    "            else:\n",
    "                logger.warning(f\"No frames extracted from video: {file_path}\")\n",
    "            \n",
    "            # Store original video file metadata \n",
    "            video_content = f\"Video file: {os.path.basename(file_path)}\"\n",
    "            # Use CLIP_TEXT to generate embedding for video metadata\n",
    "            video_embedding = self.generate_embedding(video_content, EmbeddingType.CLIP_TEXT)\n",
    "            self.add_to_milvus(video_content, video_embedding, \"video\", file_path, int(time.time()))\n",
    "                \n",
    "            # Try to extract audio\n",
    "            audio_file = self.extract_and_process_audio_from_video(file_path)\n",
    "            \n",
    "            if audio_file and os.path.exists(audio_file):\n",
    "                try:\n",
    "                    # Transcribe the audio\n",
    "                    transcription = self.transcribe_audio(audio_file)\n",
    "                    \n",
    "                    if transcription:\n",
    "                        # Generate embedding for transcription text using CLIP_TEXT\n",
    "                        text_embedding = self.generate_embedding(transcription, EmbeddingType.CLIP_TEXT)\n",
    "                        self.add_to_milvus(transcription, text_embedding, \"video_transcription\", \n",
    "                                          file_path, int(time.time()))\n",
    "                        \n",
    "                        # Generate audio embedding using CLIP_TEXT for consistency\n",
    "                        audio_content = f\"Audio from video: {os.path.basename(file_path)}\"\n",
    "                        audio_embedding = self.generate_embedding(audio_content, EmbeddingType.CLIP_TEXT)\n",
    "                        self.add_to_milvus(audio_content, audio_embedding, \n",
    "                                         \"video_audio\", file_path, int(time.time()))\n",
    "                    else:\n",
    "                        logger.warning(f\"No transcription generated for {file_path}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error transcribing audio from video {file_path}: {str(e)}\")\n",
    "                finally:\n",
    "                    # Clean up temporary audio file\n",
    "                    try:\n",
    "                        if os.path.exists(audio_file):\n",
    "                            os.remove(audio_file)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to clean up temp audio file {audio_file}: {str(e)}\")\n",
    "            else:\n",
    "                logger.info(f\"No audio extracted from video: {file_path}\")\n",
    "                \n",
    "            logger.info(f\"Completed processing video file: {file_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing video file {file_path}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def ensure_directories():\n",
    "    \"\"\"Ensure required directories exist.\"\"\"\n",
    "    directories = ['./documents', './videos', './audio', './images']\n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Ensured directory exists: {directory}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        logger.info(\"Loaded environment variables\")\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        ensure_directories()\n",
    "        \n",
    "        # Get credentials\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "            \n",
    "        github_token = os.getenv('GITHUB_TOKEN')\n",
    "        microsoft_client_id = os.getenv('MICROSOFT_CLIENT_ID')\n",
    "        microsoft_client_secret = os.getenv('MICROSOFT_CLIENT_SECRET')\n",
    "        microsoft_tenant_id = os.getenv('MICROSOFT_TENANT_ID')\n",
    "        \n",
    "        # Get config file path from environment variable\n",
    "        config_file = os.getenv('DATA_TYPE_CONFIG')\n",
    "        if not config_file or not os.path.exists(config_file):\n",
    "            logger.warning(\"No config file specified or file not found, using default configuration\")\n",
    "            config_file = None\n",
    "        else:\n",
    "            logger.info(f\"Using configuration from {config_file}\")\n",
    "        \n",
    "        # Initialize data ingestion with retry mechanism\n",
    "        max_retries = 3\n",
    "        retry_delay = 10  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"Initializing DataIngestion (attempt {attempt+1}/{max_retries})...\")\n",
    "                ingestion = DataIngestion(\n",
    "                    openai_api_key=openai_api_key,\n",
    "                    github_token=github_token,\n",
    "                    microsoft_client_id=microsoft_client_id,\n",
    "                    microsoft_client_secret=microsoft_client_secret,\n",
    "                    microsoft_tenant_id=microsoft_tenant_id,\n",
    "                    config_file=config_file\n",
    "                )\n",
    "                logger.info(\"DataIngestion initialized successfully\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize DataIngestion: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    logger.error(f\"Failed to initialize DataIngestion after {max_retries} attempts\")\n",
    "                    raise\n",
    "        \n",
    "        # Process all data sources according to configuration\n",
    "        logger.info(\"Starting automatic data ingestion based on configuration...\")\n",
    "        total_processed = 0\n",
    "        \n",
    "        # Authenticate with OneNote at the beginning if enabled\n",
    "        process_onenote = False\n",
    "        if all([microsoft_client_id, microsoft_client_secret, microsoft_tenant_id]) and \\\n",
    "           ingestion.config.sources.get(\"onenote\", {}).enabled:\n",
    "            logger.info(\"Authenticating with OneNote...\")\n",
    "            try:\n",
    "                ingestion.onenote_token = ingestion.get_onenote_access_token()\n",
    "                logger.info(\"OneNote authentication successful\")\n",
    "                process_onenote = True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"OneNote authentication failed: {str(e)}\")\n",
    "                logger.info(\"Continuing with other data sources\")\n",
    "        \n",
    "        # Process local directories based on configuration\n",
    "        if ingestion.config.sources.get(\"local\", {}).enabled:\n",
    "            logger.info(\"Processing local files...\")\n",
    "            data_directories = ['./documents', './videos', './audio', './images']\n",
    "            \n",
    "            for directory in data_directories:\n",
    "                if os.path.exists(directory):\n",
    "                    files_count = len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
    "                    if files_count > 0:\n",
    "                        logger.info(f\"Processing {files_count} files in {directory}...\")\n",
    "                        processed = ingestion.process_directory(directory)\n",
    "                        total_processed += processed\n",
    "                        logger.info(f\"Processed {processed} files from {directory}\")\n",
    "                    else:\n",
    "                        logger.info(f\"No files found in {directory}, skipping\")\n",
    "                else:\n",
    "                    logger.info(f\"Directory {directory} does not exist, skipping\")\n",
    "        else:\n",
    "            logger.info(\"Local file processing is disabled in configuration\")\n",
    "        \n",
    "        # Process GitHub repositories if enabled in config and token is available\n",
    "        if ingestion.config.sources.get(\"github\", {}).enabled and github_token:\n",
    "            logger.info(\"Processing GitHub repositories...\")\n",
    "            github_processed = ingestion.process_github_repo()\n",
    "            total_processed += github_processed\n",
    "            logger.info(f\"Processed {github_processed} files from GitHub\")\n",
    "        elif ingestion.config.sources.get(\"github\", {}).enabled:\n",
    "            logger.warning(\"GitHub processing is enabled but GitHub token is not available\")\n",
    "        \n",
    "        # Process OneNote content if enabled and authentication succeeded\n",
    "        if process_onenote and ingestion.config.sources.get(\"onenote\", {}).enabled:\n",
    "            logger.info(\"Processing OneNote content...\")\n",
    "            onenote_processed = ingestion.process_onenote_content()\n",
    "            total_processed += onenote_processed\n",
    "            logger.info(f\"Processed {onenote_processed} pages from OneNote\")\n",
    "        \n",
    "        if total_processed > 0:\n",
    "            logger.info(f\"Data ingestion complete. Total items processed: {total_processed}\")\n",
    "        else:\n",
    "            logger.warning(\"No items were processed. Check if your configuration has enabled sources and if there are files to process.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up any remaining temporary files\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        for file in os.listdir(temp_dir):\n",
    "            if file.startswith('temp_'):\n",
    "                try:\n",
    "                    os.remove(os.path.join(temp_dir, file))\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to remove temporary file {file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Starting Personal RAG data ingestion process...\")\n",
    "    logger.info(\"Automatically processing all data sources according to configuration in data_types_config.json\")\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
